<!DOCTYPE html>
<html>
<head>

<meta name="twitter:title" content="深層学習前半レポート"/>
<meta name="twitter:description" content="深層学習前半の要点のまとめと実装演習結果へのリンクです。"/>
<meta name="twitter:image" content="https://ontheroad2021.github.io/RabbitChallenge/images/RabbitChallengeImage.png"/>
<meta name="twitter:card" content="summary"/>

<style>
body {
	background-color: white;
}
table {
	border-collapse: collapse;
	background-color: lightgray;
}
th, td {
	border: 1px solid #333;
}
td {
	text-align: center;
}

.group { 
	background: powderblue;
	width: 100%;	
}

/*
.group:hover {
    background-color: lightsteelblue;
}
*/

.g1 { width:  5% }
.g2 { width: 10%; text-align: left; }
.g3 { width: 15%; text-align: left; }
.g4 { width: 25%; text-align: left; }
.g5 { width: 10%; text-align: }
.g6 { width: 10%; text-align: }
.g7 { width: 10%; text-align: right; }

.header {
	padding: 10px;
	text-align: center;
	background:steelblue;
	color: black;
	font-size: 20px;
	border: 1px solid #333;

	overflow-y: auto; 
    position: sticky; 
	top: 0; 
}
.left { display: table-cell; text-align: left; }
.right { display: table-cell; text-align: right; }

#navbar {
  padding: 0px;
  text-align: center;
  background-color: steelblue;
  color: black;
  font-size: 20px;
  /* border: 1px solid #333; */

  position: sticky; /* Make it stick/fixed */
  top: 0; /* Stay on top */
  transition: top 1.0s; /* Transition effect when sliding down (and up) */
}

</style>
</head>

<body>
	<div id="navbar">
		<table class="header">
			<tr>
				<th colspan="8"><h2>深層学習前半レポート</h2></th>
			</tr>
			<tr>
				<th width="5%">ビデオ視聴学習者提出区分け</th>
				<th width="10%">科目</th>
				<th width="15%">章タイトル</th>
				<th width="25%" class="left">１点１００文字以上で要点のまとめ</th>
				<th width="10%" class="left">実装演習結果キャプチャーまたはサマリーと考察</th>
				<th width="10%" class="left">「確認テスト」など、自身の考察結果</th>
				<th width="10%" class="left">演習問題や参考図書、修了課題など関連記事レポートによる加点</th>
			</tr>
		</table	>
	</div>
		
	<table class="group">
		<tr>
			<td class="g1" rowspan="10">【c】</td>
			<td class="g2" rowspan="5" class="left">深層学習day１</td>
			<td class="g3">
				Section１：入力層～中間層
			</td>
			<td class="g4">
				<ul>
	<li>ｗは傾きを変える事ができる。</li>

	<li>ｂは切片を変える事ができる。</li>

	<li>傾きと切片が決まれば、１次関数の式は一意に決定される。</li>
				</ul>
			</td>
			<td class="g5">
				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/3_1_1_1_1_1_forward_propagation.ipynb" target="_blank">順伝播</a>
			</td>
			<td class="g6">
				<a href="https://github.com/ontheroad2021/RabbitChallenge/blob/main/3_1_1_2_Review_Test.md" target="_blank">確認テスト（入力層～中間層）</a>
			</td>
			<td class="g7">
				１点
			</td>
		</tr>
		<tr>
			<td class="left">
				Section２：活性化関数
			</td>
			<td class="left">
				<ul>

	<li>活性化関数は非線形である事が大切。</li>

	<li>線形な関数は加法性（additivity）と斉次性（homogeneity）を満たす。</li>

	<li>非線形な関数はそれらを満たさない。</li>

	<li>ステップ関数。課題：０－１の間を表現できず、線形分離可能なものしか学習できなかった。</li>

	<li>シグモイド関数。課題：勾配消失問題。</li>

	<li>RELU関数。勾配消失問題の回避とスパース化（モデルの中身がシンプルになる）に貢献する。</li>
	
				</ul>
			</td>
			<td>
				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/3_1_2_1_1_1_forward_propagation_after.ipynb" target="_blank">順伝播</a>
			</td>
			<td>
				<a href="https://github.com/ontheroad2021/RabbitChallenge/blob/main/3_1_2_2_Review_Test.md" target="_blank">確認テスト（活性化関数）</a>
			</td>
			<td class="right">
				１点			
			</td>
		</tr>
		<tr>
			<td class="left">
				Section３：出力層
			</td>
			<td class="left">
				<ul>

	<li>中間層は次の層の入力として適切なものを出力する。</li>
	
	<li>出力層の出力は目的に合致したものである必要がある（例えば、各クラスの」確率）。</li>
				
	<li>「誤差関数」はそれぞれのニューラルネットワークの出力結果と正解データを比較する事によって「どのくらいあっていたか」を見る。</li>
				
	<li>「活性化関数」は出力層と中間層のものとでは目的が違う。</li>
				
	<li>恒等写像	回帰	二乗誤差</li>
				
	<li>シグモイド関数	二値分類	交差エントロピー</li>
				
	<li>ソフトマックス関数	多クラス分類	交差エントロピー</li>
				
	<li>交差エントロピーは分類問題の誤差関数としては非常に良く使用する。</li>

				</ul>
			</td>
			<td>
				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/3_1_3_1_1_2_back_propagation.ipynb" target="_blank">逆伝播</a>	
			</td>
			<td>
				<a href="https://github.com/ontheroad2021/RabbitChallenge/blob/main/3_1_3_2_Review_Test.md" target="_blank">確認テスト（出力層）</a>
			</td>
			<td class="right">
				１点			
			</td>
		</tr>
		<tr>
			<td class="left">
				Section４：勾配降下法
			</td>
			<td class="left">
				<ul>

	<li>「勾配降下法」</li>
					<ul>

	<li>ニューラルネットワークを学習させる手法の事。</li>

	<li>学習率εがどのように学習に影響するのか。</li>

	<li>大きすぎる場合：発散。</li>

	<li>小さい場合：収束するまでに時間がかかる。</li>

	<li>エポック：重み更新一回のサイクルの事。</li>

					</ul>

	<li>「確率的勾配降下法（SGD）」</li>

					<ul>

	<li>望まない局所極小解に収束するリスクの軽減。</li>

	<li>オンライン学習ができる（⇔バッチ学習）</li>

					</ul>

	<li>「ミニバッチ勾配降下法」</li>

					<ul>

	<li>オンライン学習の特徴を上手くバッチ学習で使用できる様にした手法。</li>

	<li>誤差は誤差の和をミニバッチの数で割ったもの。</li>

	<li>CPUを利用したスレッドの並列化やGPUを利用したSIMD（Single Instruction Multiple Data）並列化。</li>

					</ul>
				</ul>
			</td>
			<td>
				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/3_1_4_1_1_3_stochastic_gradient_descent.ipynb" target="_blank">確率的勾配降下法</a>
			</td>
			<td>
				<a href="https://github.com/ontheroad2021/RabbitChallenge/blob/main/3_1_4_2_Review_Test.md" target="_blank">確認テスト（勾配降下法）</a>
			</td>
			<td class="right">
				１点			
			</td>
		</tr>
		<tr>
			<td class="left">
				Section５：誤差逆伝播法
			</td>
			<td class="left">
				<ul>

<li>誤差勾配の計算</li>					

<li>数値微分⇒計算量が非常に大きくなる。</li>					
					
<li>誤差逆伝播法⇒算出される誤差を、出力側から順に微分し、前の層へと伝播。最小限の計算で各パラメータでの微分値を解析的に計算する方法。</li>

				</ul>
			</td>
			<td>
				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/3_1_5_1_1_4_1_mnist_sample.ipynb" target="_blank">MNIST</a>
			</td>
			<td>
				<a href="https://github.com/ontheroad2021/RabbitChallenge/blob/main/3_1_5_2_Review_Test.md" target="_blank">確認テスト（誤差逆伝播法）</a>
			</td>
			<td class="right">
				１点			
			</td>
		</tr>
			<td rowspan="5" class="left">深層学習day２</td>
			<td class="left">
				Section１：勾配消失問題
			</td>
			<td class="left">
				<ul>

<li>勾配消失問題：中間層が増加した時、誤差逆伝播法というニューラルネットワークの学習法が上手く行かなくなる。</li>

<li>勾配消失の解決方法</li>

					<ul>
				
<li>活性化関数の選択：ReLU関数、勾配消失問題の回避とスパース化に貢献する事で良い結果をもたらしている。</li>
				
<li>重みの初期値設定</li>
				
<li>バッチ正規化</li>

					</ul>
				</ul>
			</td>
			<td>
				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/3_2_1_1_2_1_network_modified.ipynb" target="_blank">ネットワーク（修正）</a><br>

				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/3_2_1_1_2_2_1_vanishing_gradient.ipynb" target="_blank">勾配消失問題</a><br>

				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/3_2_1_1_2_2_2_vanishing_gradient_modified.ipynb" target="_blank">勾配消失問題（修正）</a><br>

				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/3_2_1_1_2_3_batch_normalization.ipynb" target="_blank">バッチ正規化</a><br>

			</td>
			<td>
				<a href="https://github.com/ontheroad2021/RabbitChallenge/blob/main/3_2_1_2_Review_Test.md" target="_blank">確認テスト（勾配消失問題）</a>
			</td>
			<td class="right">
				１点
			</td>
		</tr>	
		<tr>
			<td class="left">
				Section２：学習率最適化手法
			</td>
			<td class="left">
				<ul>

<li>学習率最適化のアニメーション（<a href="https://github.com/Jaewan-Yun/optimizer-visualization" target="_blank">*</a>）</li>					

<li>鞍点問題のアニメーション（<a href="https://blog.albert2005.co.jp/2016/08/24/deeplearning%e3%81%8c%e3%81%aa%e3%81%9c%e3%81%86%e3%81%be%e3%81%8f%e5%ad%a6%e7%bf%92%e5%87%ba%e6%9d%a5%e3%82%8b%e3%81%ae%e3%81%8b/" target="_blank">*</a>）</li>					
					
<li>モメンタム：SDGのジグザグ運転に対して株価の移動平均のような動きをする。</li>					
					
<li>AdaGrad：緩やかな斜面に強い。</li>					
					
<li>RMSProp（AdaGradを改良）；鞍点問題をスムーズに解消できるようにした試みになる。</li>					
					<ul>					

<li>α：どれくらい前回の経験を生かすか。</li>					
					
<li>AdaGradのようにハイパーパラメータの調整が必要な場合が少ない。</li>					

					</ul>

<li>Adam：優秀な最適化手法（Optimizer）モメンタム及びRMSPropのメリットを学んだアルゴリズムである。</li>										

				</ul>
			</td>
			<td>
				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/3_2_2_1_2_4_optimizer_after.ipynb" target="_blank">学習率最適化手法</a>
			</td>
			<td>
				<a href="https://github.com/ontheroad2021/RabbitChallenge/blob/main/3_2_2_2_Review_Test.md" target="_blank">確認テスト（学習率最適化手法）</a>
			</td>
			<td class="right">
				１点			
			</td>
		</tr>
		<tr>
			<td class="left">
				Section３：過学習
			</td>
			<td class="left">
				<ul>

<li>過学習：テスト誤差と訓練誤差とで学習曲線が乖離する事。

<li>正則化：ネットワークの自由度（層数、ノード数、パラメータの値等）を制約する事。</li>

					<ul>				

<li>L1正則化（Lasso回帰）マンハッタン距離（P1ノルム）</li>

<li>L2正則化（Ridge回帰）ユークリッド距離（P2ノルム）</li>				
				
					</ul>

<li>ドロップアウト</li>				
				
<li>Weight decay（荷重減衰）</li>

					<ul>
				
<li>重みが大きい値は、学習において重要な値であるが、重みが大きいと過学習が起こる。</li>

<li>過学習がおこりそうな重みの大きさ以下で重みをコントロールし、かつ重みの大きさにはばらつきを出す必要がある。</li>				

					</ul>
				</ul>		
			</td>
			<td>
				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/3_2_3_1_2_5_overfiting.ipynb" target="_blank">過学習</a>
			</td>
			<td class="right">
				<a href="https://github.com/ontheroad2021/RabbitChallenge/blob/main/3_2_3_2_Review_Test.md" target="_blank">確認テスト（過学習）</a>
			</td>
			<td class="right">
				１点			
			</td>
		</tr>
		<tr>
			<td class="left">
				Section４：畳み込みニューラルネットワークの概念
			</td>
			<td class="right">
				１点
			</td>
			<td>
				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/3_2_4_1_2_6_simple_convolution_network_after.ipynb" target="_blank">畳み込みネットワーク</a>
			</td>
			<td class="right">
				１点
			</td>
			<td class="right">
				１点			
			</td>
		</tr>
		<tr>
			<td class="left">
				Section５：最新のＣＮＮ
			</td>
			<td class="right">
				１点
			</td>
			<td class="right">
				１点
			</td>
			<td class="right">
				１点
			</td>
			<td class="right">
				１点			
			</td>
		</tr>	
	</table>

	<script>
		/* When the user scrolls down, hide the navbar. When the user scrolls up, show the navbar */
		var prevScrollpos = window.pageYOffset;
		window.onscroll = function() {
		var currentScrollPos = window.pageYOffset;
		  if (prevScrollpos > currentScrollPos) {
			document.getElementById("navbar").style.top = "0";
		  } else {
			document.getElementById("navbar").style.top = "-250px";
		  }
		  prevScrollpos = currentScrollPos;
		}
	</script>

</body>
</html>
