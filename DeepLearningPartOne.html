<!DOCTYPE html>
<html>
<head>

<meta name="twitter:title" content="深層学習前半レポート"/>
<meta name="twitter:description" content="深層学習前半の要点のまとめと実装演習結果へのリンクです。"/>
<meta name="twitter:image" content="https://ontheroad2021.github.io/RabbitChallenge/images/RabbitChallengeImage.png"/>
<meta name="twitter:card" content="summary"/>

<style>
body {
	background-color: white;
}
table {
	border-collapse: collapse;
	background-color: lightgray;
}
th, td {
	border: 1px solid #333;
}
td {
	text-align: center;
}

.group { 
	background: powderblue;
	width: 100%;	
}

/*
.group:hover {
    background-color: lightsteelblue;
}
*/

.g1 { width:  5% }
.g2 { width: 10%; text-align: left; }
.g3 { width: 15%; text-align: left; }
.g4 { width: 25%; text-align: left; }
.g5 { width: 10%; text-align: }
.g6 { width: 10%; text-align: }
.g7 { width: 10%; text-align: right; }

.header {
	padding: 10px;
	text-align: center;
	background:steelblue;
	color: black;
	font-size: 20px;
	border: 1px solid #333;

	overflow-y: auto; 
    position: sticky; 
	top: 0; 
}
.left { display: table-cell; text-align: left; }
.right { display: table-cell; text-align: right; }

#navbar {
  padding: 0px;
  text-align: center;
  background-color: steelblue;
  color: black;
  font-size: 20px;
  /* border: 1px solid #333; */

  position: sticky; /* Make it stick/fixed */
  top: 0; /* Stay on top */
  transition: top 1.0s; /* Transition effect when sliding down (and up) */
}

</style>
</head>

<body>
	<div id="navbar">
		<table class="header">
			<tr>
				<th colspan="8"><h2>深層学習前半レポート</h2></th>
			</tr>
			<tr>
				<th width="5%">ビデオ視聴学習者提出区分け</th>
				<th width="10%">科目</th>
				<th width="15%">章タイトル</th>
				<th width="25%" class="left">１点１００文字以上で要点のまとめ</th>
				<th width="10%" class="left">実装演習結果キャプチャーまたはサマリーと考察</th>
				<th width="10%" class="left">「確認テスト」など、自身の考察結果</th>
				<th width="10%" class="left">演習問題や参考図書、修了課題など関連記事レポートによる加点</th>
			</tr>
		</table	>
	</div>
		
	<table class="group">
		<tr>
			<td class="g1" rowspan="10">【c】</td>
			<td class="g2" rowspan="5" class="left">深層学習day１</td>
			<td class="g3">
				Section１：入力層～中間層
			</td>
			<td class="g4">
				<ul>
	<li>ｗは傾きを変える事ができる。</li>

	<li>ｂは切片を変える事ができる。</li>

	<li>傾きと切片が決まれば、１次関数の式は一意に決定される。</li>
				</ul>
			</td>
			<td class="g5">
				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/3_1_1_1_1_1_forward_propagation.ipynb" target="_blank">順伝播</a>
			</td>
			<td class="g6">
				<a href="https://github.com/ontheroad2021/RabbitChallenge/blob/main/3_1_1_2_Review_Test.md" target="_blank">確認テスト（入力層～中間層）</a>
			</td>
			<td class="g7">
				１点
			</td>
		</tr>
		<tr>
			<td class="left">
				Section２：活性化関数
			</td>
			<td class="left">
				<ul>

	<li>活性化関数は非線形である事が大切。</li>

	<li>線形な関数は加法性（additivity）と斉次性（homogeneity）を満たす。</li>

	<li>非線形な関数はそれらを満たさない。</li>

	<li>ステップ関数。課題：０－１の間を表現できず、線形分離可能なものしか学習できなかった。</li>

	<li>シグモイド関数。課題：勾配消失問題。</li>

	<li>RELU関数。勾配消失問題の回避とスパース化（モデルの中身がシンプルになる）に貢献する。</li>
	
				</ul>
			</td>
			<td>
				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/3_1_2_1_1_1_forward_propagation_after.ipynb" target="_blank">順伝播</a>
			</td>
			<td>
				<a href="https://github.com/ontheroad2021/RabbitChallenge/blob/main/3_1_2_2_Review_Test.md" target="_blank">確認テスト（活性化関数）</a>
			</td>
			<td class="right">
				１点			
			</td>
		</tr>
		<tr>
			<td class="left">
				Section３：出力層
			</td>
			<td class="left">
				<ul>

	<li>中間層は次の層の入力として適切なものを出力する。</li>
	
	<li>出力層の出力は目的に合致したものである必要がある（例えば、各クラスの」確率）。</li>
				
	<li>「誤差関数」はそれぞれのニューラルネットワークの出力結果と正解データを比較する事によって「どのくらいあっていたか」を見る。</li>
				
	<li>「活性化関数」は出力層と中間層のものとでは目的が違う。</li>
				
	<li>恒等写像	回帰	二乗誤差</li>
				
	<li>シグモイド関数	二値分類	交差エントロピー</li>
				
	<li>ソフトマックス関数	多クラス分類	交差エントロピー</li>
				
	<li>交差エントロピーは分類問題の誤差関数としては非常に良く使用する。</li>

				</ul>
			</td>
			<td>
				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/3_1_3_1_1_2_back_propagation.ipynb" target="_blank">逆伝播</a>	
			</td>
			<td>
				<a href="https://github.com/ontheroad2021/RabbitChallenge/blob/main/3_1_3_2_Review_Test.md" target="_blank">確認テスト（出力層）</a>
			</td>
			<td class="right">
				１点			
			</td>
		</tr>
		<tr>
			<td class="left">
				Section４：勾配降下法
			</td>
			<td class="left">
				<ul>

	<li>「勾配降下法」</li>
					<ul>

	<li>ニューラルネットワークを学習させる手法の事。</li>

	<li>学習率εがどのように学習に影響するのか。</li>

	<li>大きすぎる場合：発散。</li>

	<li>小さい場合：収束するまでに時間がかかる。</li>

	<li>エポック：重み更新一回のサイクルの事。</li>

					</ul>

	<li>「確率的勾配降下法（SGD）」</li>

					<ul>

	<li>望まない局所極小解に収束するリスクの軽減。</li>

	<li>オンライン学習ができる（⇔バッチ学習）</li>

					</ul>

	<li>「ミニバッチ勾配降下法」</li>

					<ul>

	<li>オンライン学習の特徴を上手くバッチ学習で使用できる様にした手法。</li>

	<li>誤差は誤差の和をミニバッチの数で割ったもの。</li>

	<li>CPUを利用したスレッドの並列化やGPUを利用したSIMD（Single Instruction Multiple Data）並列化。</li>

					</ul>
				</ul>
			</td>
			<td>
				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/3_1_4_1_1_3_stochastic_gradient_descent.ipynb" target="_blank">確率的勾配降下法</a>
			</td>
			<td>
				<a href="https://github.com/ontheroad2021/RabbitChallenge/blob/main/3_1_4_2_Review_Test.md" target="_blank">確認テスト（勾配降下法）</a>
			</td>
			<td class="right">
				１点			
			</td>
		</tr>
		<tr>
			<td class="left">
				Section５：誤差逆伝播法
			</td>
			<td class="left">
				<ul>

<li>誤差勾配の計算</li>					

<li>数値微分⇒計算量が非常に大きくなる。</li>					
					
<li>誤差逆伝播法⇒算出される誤差を、出力側から順に微分し、前の層へと伝播。最小限の計算で各パラメータでの微分値を解析的に計算する方法。</li>

				</ul>
			</td>
			<td>
				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/3_1_5_1_1_4_1_mnist_sample.ipynb" target="_blank">MNIST</a>
			</td>
			<td>
				<a href="https://github.com/ontheroad2021/RabbitChallenge/blob/main/3_1_5_2_Review_Test.md" target="_blank">確認テスト（誤差逆伝播法）</a>
			</td>
			<td class="right">
				１点			
			</td>
		</tr>
			<td rowspan="5" class="left">深層学習day２</td>
			<td class="left">
				Section１：勾配消失問題
			</td>
			<td class="left">
				<ul>

<li>勾配消失問題：中間層が増加した時、誤差逆伝播法というニューラルネットワークの学習法が上手く行かなくなる。</li>

<li>勾配消失の解決方法</li>

					<ul>
				
<li>活性化関数の選択：ReLU関数、勾配消失問題の回避とスパース化に貢献する事で良い結果をもたらしている。</li>
				
<li>重みの初期値設定</li>
				
<li>バッチ正規化</li>

					</ul>
				</ul>
			</td>
			<td>
				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/3_2_1_1_2_1_network_modified.ipynb" target="_blank">ネットワーク（修正）</a><br>

				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/3_2_1_1_2_2_1_vanishing_gradient.ipynb" target="_blank">勾配消失問題</a><br>

				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/3_2_1_1_2_2_2_vanishing_gradient_modified.ipynb" target="_blank">勾配消失問題（修正）</a><br>

				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/3_2_1_1_2_3_batch_normalization.ipynb" target="_blank">バッチ正規化</a><br>

			</td>
			<td>
				<a href="https://github.com/ontheroad2021/RabbitChallenge/blob/main/3_2_1_2_Review_Test.md" target="_blank">確認テスト（勾配消失問題）</a>
			</td>
			<td class="right">
				１点
			</td>
		</tr>	
		<tr>
			<td class="left">
				Section２：学習率最適化手法
			</td>
			<td class="left">
				<ul>

<li>学習率最適化のアニメーション（<a href="https://github.com/Jaewan-Yun/optimizer-visualization" target="_blank">*</a>）</li>					

<li>鞍点問題のアニメーション（<a href="https://blog.albert2005.co.jp/2016/08/24/deeplearning%e3%81%8c%e3%81%aa%e3%81%9c%e3%81%86%e3%81%be%e3%81%8f%e5%ad%a6%e7%bf%92%e5%87%ba%e6%9d%a5%e3%82%8b%e3%81%ae%e3%81%8b/" target="_blank">*</a>）</li>					
					
<li>モメンタム：SDGのジグザグ運転に対して株価の移動平均のような動きをする。</li>					
					
<li>AdaGrad：緩やかな斜面に強い。</li>					
					
<li>RMSProp（AdaGradを改良）；鞍点問題をスムーズに解消できるようにした試みになる。</li>					
					<ul>					

<li>α：どれくらい前回の経験を生かすか。</li>					
					
<li>AdaGradのようにハイパーパラメータの調整が必要な場合が少ない。</li>					

					</ul>

<li>Adam：優秀な最適化手法（Optimizer）モメンタム及びRMSPropのメリットを学んだアルゴリズムである。</li>										

				</ul>
			</td>
			<td>
				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/3_2_2_1_2_4_optimizer_after.ipynb" target="_blank">学習率最適化手法</a>
			</td>
			<td>
				<a href="https://github.com/ontheroad2021/RabbitChallenge/blob/main/3_2_2_2_Review_Test.md" target="_blank">確認テスト（学習率最適化手法）</a>
			</td>
			<td class="right">
				１点			
			</td>
		</tr>
		<tr>
			<td class="left">
				Section３：過学習
			</td>
			<td class="left">
				<ul>

<li>過学習：テスト誤差と訓練誤差とで学習曲線が乖離する事。

<li>正則化：ネットワークの自由度（層数、ノード数、パラメータの値等）を制約する事。</li>

					<ul>				

<li>L1正則化（Lasso回帰）マンハッタン距離（P1ノルム）</li>

<li>L2正則化（Ridge回帰）ユークリッド距離（P2ノルム）</li>				
				
					</ul>

<li>ドロップアウト</li>				
				
<li>Weight decay（荷重減衰）</li>

					<ul>
				
<li>重みが大きい値は、学習において重要な値であるが、重みが大きいと過学習が起こる。</li>

<li>過学習がおこりそうな重みの大きさ以下で重みをコントロールし、かつ重みの大きさにはばらつきを出す必要がある。</li>				

					</ul>
				</ul>		
			</td>
			<td>
				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/3_2_3_1_2_5_overfiting.ipynb" target="_blank">過学習</a>
			</td>
			<td>
				<a href="https://github.com/ontheroad2021/RabbitChallenge/blob/main/3_2_3_2_Review_Test.md" target="_blank">確認テスト（過学習）</a>
			</td>
			<td class="right">
				１点			
			</td>
		</tr>
		<tr>
			<td class="left">
				Section４：畳み込みニューラルネットワークの概念
			</td>
			<td class="left">
				<ul>

<li>CNN：画像処理を行う時に良く用いられるニューラルネットワーク。次元間で繋がりのあるデータならなんでも扱える。</li>

<li>LeNet：畳み込みニューラルネットワークの代表的なものの1つ。</li>
				
<li>畳み込み層：ニューラルネットワークでは変換を行うというのはデータから特徴を読み取るという事に直接該当します。</li>
				
<li>全結合層での学習：全体の構成＝次元の繋がりを保つ（特徴の抽出）＋人間が欲しい結果（全結合層）</li>
				
<li>畳み込み層：３次元の空間情報も学習できるような層である。</li>

					<ul>

<li>フィルター（全結合でいう重み）</li>

<li>バイアス（畳み込みの演算概念）</li>

<li>パディング（畳み込み演算を何回も繰り返すと画像が小さくなってしまう）</li>

<li>ストライド（ストライドの数はダイレクトに出力画像を小さくする要因になる）</li>

<li>チャネル（フィルターの数）</li>

					</ul>
				
<li>畳み込み演算をしたくなった理由：全結合では、縦、横、チャネルの３次元データ（特徴）が１次元のデータとして処理される。それでは上手く画像が持つ情報から特徴を抽出する事ができない。</li>
				
<li>im2col:畳み込み演算を効率的に行うためのアルゴリズム。</li>
				
<li>プーリング層：重みがない。Max Pooling, Avg Pooling：畳み込み演算と同じように少しづつずれながら画像を読み込む。</li>

				</ul>
			</td>
			<td>
				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/3_2_4_1_2_6_simple_convolution_network_after.ipynb" target="_blank">畳み込みネットワーク</a>
			</td>
			<td>
				確認テスト(畳み込みニューラルネットワークの概念)
			</td>
			<td class="right">
				１点			
			</td>
		</tr>
		<tr>
			<td class="left">
				Section５：最新のＣＮＮ
			</td>
			<td class="left">
				<ul>

<li>AlexNet：比較的初期の小さなニューラルネット</li>
					
<li>モデルの構造：５層の畳み込み層、及びプーリング層等、それに続く３層の全結合層から構成される。</li>
					
<li>畳み込み演算の部分から全結合層に至る部分について：</li>
										
<li>[13, 13, 256]の画像</li>
					
					<ul>

<li>Flatten：横１列にずらっと並べるだけ[43264]。初期のニューラルネットワークでの１列の並べ替えでは非常に良く行われている。</li>
					
<li>Global Max Pooling：[13*13]をあたかも１つのフィルターのように見立て、Maxを使用する。[256]にまで圧縮される。</li>
					
<li>Global Avg Pooling：上と同様、Avg。</li>
					
					</ul>
					
<li>後２つは何故か非常に上手くいく。一気に数値を減らせる割に非常に効率的に特徴量を抽出して認識精度の向上をはかる事ができる。</li>
					
				</ul>					
			</td>
			<td class="right">
				１点
			</td>
			<td class="right">
				１点
			</td>
			<td class="right">
				１点			
			</td>
		</tr>	
	</table>

	<script>
		/* When the user scrolls down, hide the navbar. When the user scrolls up, show the navbar */
		var prevScrollpos = window.pageYOffset;
		window.onscroll = function() {
		var currentScrollPos = window.pageYOffset;
		  if (prevScrollpos > currentScrollPos) {
			document.getElementById("navbar").style.top = "0";
		  } else {
			document.getElementById("navbar").style.top = "-250px";
		  }
		  prevScrollpos = currentScrollPos;
		}
	</script>

</body>
</html>
