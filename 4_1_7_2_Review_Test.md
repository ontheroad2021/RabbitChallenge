# 深層学習 day３ Section７：Ａｔｔｅｎｔｉｏｎ　Ｍｅｃｈａｎｉｓｍ

## 確認テスト１

ＲＮＮとｗｏｒｄ２ｖｅｃ、ｓｅｑ２ｓｅｑとＡｔｔｅｎｔｉｏｎの違いを簡潔に述べよ。

解答：

ＲＮＮは時系列データを処理するのに適したネットワークです。  
ｗｏｒｄ２ｖｅｃは単語の分散表現ベクトルを得る手法です。   
ｓｅｑ２ｓｅｑは１つの時系列データから別の時系列データを得るネットワークです。  
Ａｔｔｅｎｔｉｏｎ Ｍｅｃｈａｎｉｓｍ は時系列データの中身に対して関連性に重みを付ける手法になる。


ＬＳＴＭは入力ゲート、出力ゲート、忘却ゲートそしてＣＥＣ、それぞれ４つの部品を持つ事で構成されていた。  
そのため、ＬＳＴＭではパラメータ数が多く、計算負荷が高くなるという課題があった。  
その中にあるＣＥＣであるが、問題点としては学習能力が無い事が挙げられる。（そのために３つのゲートを周りに付け、学習機能を持たせている、というのがＣＥＣの根本的な動きになっている。）  

   

## 確認テスト２

ＬＳＴＭとＧＲＵの違いを簡潔に述べよ。

解答：

- ＬＳＴＭには入力ゲート、出力ゲート、忘却ゲートの３つのゲートとＣＥＣがある。  
１方のＧＲＵにはＣＥＣが無く更新ゲートとリセットゲートを持つ。

- ＬＳＴＭはパラメータが多く、ＧＲＵはパラメータが少ない。

- 結果的にＬＳＴＭよりＧＲＵの方が計算量は少ない（重要）。


