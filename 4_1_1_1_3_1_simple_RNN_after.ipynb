{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "3_1_simple_RNN_after.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cNl2QA_Rnv5"
      },
      "source": [
        "# 準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkwjN1jNVAYy"
      },
      "source": [
        "## Googleドライブのマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvFXpiH3EVC1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e9e8b39-a57f-4416-e5e1-d56adafda6df"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ub7RYdeY6pK"
      },
      "source": [
        "## sys.pathの設定"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oql7L19rEsWi"
      },
      "source": [
        "以下では，Googleドライブのマイドライブ直下にDNN_codeフォルダを置くことを仮定しています．必要に応じて，パスを変更してください．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ic2JzkvFX59"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/DNN_code')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzGmsHRwO-bi"
      },
      "source": [
        "# simple RNN after\n",
        "### バイナリ加算"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "KNSG0aKXO-bk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "af943dbf-33a4-41f3-b47a-f6d0d5e352f2"
      },
      "source": [
        "import numpy as np\n",
        "from common import functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def d_tanh(x):\n",
        "    return 1/(np.cosh(x) ** 2)\n",
        "\n",
        "# データを用意\n",
        "# 2進数の桁数\n",
        "binary_dim = 8\n",
        "# 最大値 + 1\n",
        "largest_number = pow(2, binary_dim)\n",
        "# largest_numberまで2進数を用意\n",
        "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
        "\n",
        "input_layer_size = 2\n",
        "hidden_layer_size = 16\n",
        "output_layer_size = 1\n",
        "\n",
        "weight_init_std = 1\n",
        "learning_rate = 0.1\n",
        "\n",
        "iters_num = 10000\n",
        "plot_interval = 100\n",
        "\n",
        "# ウェイト初期化 (バイアスは簡単のため省略)\n",
        "W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
        "W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
        "W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n",
        "# Xavier\n",
        "# W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size))\n",
        "# W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "# W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "# He\n",
        "# W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size)) * np.sqrt(2)\n",
        "# W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "# W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "\n",
        "\n",
        "# 勾配\n",
        "W_in_grad = np.zeros_like(W_in)\n",
        "W_out_grad = np.zeros_like(W_out)\n",
        "W_grad = np.zeros_like(W)\n",
        "\n",
        "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "y = np.zeros((output_layer_size, binary_dim))\n",
        "\n",
        "delta_out = np.zeros((output_layer_size, binary_dim))\n",
        "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "\n",
        "all_losses = []\n",
        "\n",
        "for i in range(iters_num):\n",
        "    \n",
        "    # A, B初期化 (a + b = d)\n",
        "    a_int = np.random.randint(largest_number/2)\n",
        "    a_bin = binary[a_int] # binary encoding\n",
        "    b_int = np.random.randint(largest_number/2)\n",
        "    b_bin = binary[b_int] # binary encoding\n",
        "    \n",
        "    # 正解データ\n",
        "    d_int = a_int + b_int\n",
        "    d_bin = binary[d_int]\n",
        "    \n",
        "    # 出力バイナリ\n",
        "    out_bin = np.zeros_like(d_bin)\n",
        "    \n",
        "    # 時系列全体の誤差\n",
        "    all_loss = 0    \n",
        "    \n",
        "    # 時系列ループ\n",
        "    for t in range(binary_dim):\n",
        "        # 入力値\n",
        "        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n",
        "        # 時刻tにおける正解データ\n",
        "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
        "        \n",
        "        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
        "        z[:,t+1] = functions.sigmoid(u[:,t+1])\n",
        "#         z[:,t+1] = functions.relu(u[:,t+1])\n",
        "#         z[:,t+1] = np.tanh(u[:,t+1])    \n",
        "        y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "\n",
        "\n",
        "        #誤差\n",
        "        loss = functions.mean_squared_error(dd, y[:,t])\n",
        "        \n",
        "        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_sigmoid(y[:,t])        \n",
        "        \n",
        "        all_loss += loss\n",
        "\n",
        "        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n",
        "    \n",
        "    \n",
        "    for t in range(binary_dim)[::-1]:\n",
        "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
        "\n",
        "        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])\n",
        "#         delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_relu(u[:,t+1])\n",
        "#         delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * d_tanh(u[:,t+1])    \n",
        "\n",
        "        # 勾配更新\n",
        "        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n",
        "        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n",
        "        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n",
        "    \n",
        "    # 勾配適用\n",
        "    W_in -= learning_rate * W_in_grad\n",
        "    W_out -= learning_rate * W_out_grad\n",
        "    W -= learning_rate * W_grad\n",
        "    \n",
        "    W_in_grad *= 0\n",
        "    W_out_grad *= 0\n",
        "    W_grad *= 0\n",
        "    \n",
        "\n",
        "    if(i % plot_interval == 0):\n",
        "        all_losses.append(all_loss)        \n",
        "        print(\"iters:\" + str(i))\n",
        "        print(\"Loss:\" + str(all_loss))\n",
        "        print(\"Pred:\" + str(out_bin))\n",
        "        print(\"True:\" + str(d_bin))\n",
        "        out_int = 0\n",
        "        for index,x in enumerate(reversed(out_bin)):\n",
        "            out_int += x * pow(2, index)\n",
        "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n",
        "        print(\"------------\")\n",
        "\n",
        "lists = range(0, iters_num, plot_interval)\n",
        "plt.plot(lists, all_losses, label=\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iters:0\n",
            "Loss:1.235827512823427\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 1 1 0 0 1]\n",
            "87 + 34 = 255\n",
            "------------\n",
            "iters:100\n",
            "Loss:0.9113542269671475\n",
            "Pred:[0 0 0 0 1 0 0 1]\n",
            "True:[1 0 0 0 1 0 1 1]\n",
            "19 + 120 = 9\n",
            "------------\n",
            "iters:200\n",
            "Loss:1.0280911867626465\n",
            "Pred:[0 0 0 0 1 0 0 1]\n",
            "True:[0 1 1 0 1 1 0 1]\n",
            "54 + 55 = 9\n",
            "------------\n",
            "iters:300\n",
            "Loss:0.8142823761693766\n",
            "Pred:[1 1 0 1 1 1 0 1]\n",
            "True:[1 1 0 1 1 1 1 0]\n",
            "103 + 119 = 221\n",
            "------------\n",
            "iters:400\n",
            "Loss:1.1302271024853978\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[0 1 0 0 0 1 1 1]\n",
            "11 + 60 = 255\n",
            "------------\n",
            "iters:500\n",
            "Loss:1.0165172361120909\n",
            "Pred:[1 1 1 1 1 1 1 0]\n",
            "True:[1 1 0 1 1 1 0 0]\n",
            "125 + 95 = 254\n",
            "------------\n",
            "iters:600\n",
            "Loss:0.9916812343308756\n",
            "Pred:[0 0 1 1 1 1 1 1]\n",
            "True:[0 1 0 0 1 1 0 1]\n",
            "46 + 31 = 63\n",
            "------------\n",
            "iters:700\n",
            "Loss:0.9906842333779433\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 1 0 1 0]\n",
            "109 + 77 = 0\n",
            "------------\n",
            "iters:800\n",
            "Loss:0.9890274773539014\n",
            "Pred:[0 0 0 0 1 0 1 0]\n",
            "True:[1 0 0 0 0 1 0 0]\n",
            "79 + 53 = 10\n",
            "------------\n",
            "iters:900\n",
            "Loss:0.9805535875853497\n",
            "Pred:[0 0 1 0 1 0 0 0]\n",
            "True:[1 0 1 1 0 1 0 0]\n",
            "95 + 85 = 40\n",
            "------------\n",
            "iters:1000\n",
            "Loss:0.7570545816003469\n",
            "Pred:[0 1 0 1 0 0 0 0]\n",
            "True:[0 1 1 0 0 0 0 0]\n",
            "40 + 56 = 80\n",
            "------------\n",
            "iters:1100\n",
            "Loss:0.9219616698980465\n",
            "Pred:[0 1 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 0 1 0]\n",
            "105 + 1 = 64\n",
            "------------\n",
            "iters:1200\n",
            "Loss:0.8931753387498305\n",
            "Pred:[0 1 0 1 1 1 1 0]\n",
            "True:[0 0 1 1 1 1 0 0]\n",
            "39 + 21 = 94\n",
            "------------\n",
            "iters:1300\n",
            "Loss:0.9204606862038951\n",
            "Pred:[1 0 0 1 1 1 1 0]\n",
            "True:[1 0 1 1 0 0 0 0]\n",
            "75 + 101 = 158\n",
            "------------\n",
            "iters:1400\n",
            "Loss:0.8113301420288216\n",
            "Pred:[0 1 1 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 1 0 0]\n",
            "34 + 82 = 96\n",
            "------------\n",
            "iters:1500\n",
            "Loss:1.0508461515025622\n",
            "Pred:[0 1 1 1 1 1 0 1]\n",
            "True:[0 1 1 0 1 0 0 1]\n",
            "11 + 94 = 125\n",
            "------------\n",
            "iters:1600\n",
            "Loss:0.9776272789945184\n",
            "Pred:[0 1 1 0 1 1 0 1]\n",
            "True:[0 1 1 1 0 0 0 1]\n",
            "51 + 62 = 109\n",
            "------------\n",
            "iters:1700\n",
            "Loss:1.0708105315122884\n",
            "Pred:[0 1 0 1 1 1 1 0]\n",
            "True:[0 1 1 0 0 0 0 0]\n",
            "69 + 27 = 94\n",
            "------------\n",
            "iters:1800\n",
            "Loss:0.8209522903058148\n",
            "Pred:[1 1 0 1 1 0 0 1]\n",
            "True:[1 0 0 1 1 0 0 1]\n",
            "108 + 45 = 217\n",
            "------------\n",
            "iters:1900\n",
            "Loss:0.5667645115941734\n",
            "Pred:[0 1 1 0 0 1 0 1]\n",
            "True:[0 1 1 0 0 1 0 1]\n",
            "17 + 84 = 101\n",
            "------------\n",
            "iters:2000\n",
            "Loss:0.5076003760503002\n",
            "Pred:[0 0 1 0 0 1 1 0]\n",
            "True:[0 0 1 0 0 1 1 0]\n",
            "30 + 8 = 38\n",
            "------------\n",
            "iters:2100\n",
            "Loss:0.47081904436236177\n",
            "Pred:[0 0 1 1 1 1 0 0]\n",
            "True:[0 0 1 1 1 1 0 0]\n",
            "4 + 56 = 60\n",
            "------------\n",
            "iters:2200\n",
            "Loss:1.003812866365913\n",
            "Pred:[0 1 1 1 0 1 1 1]\n",
            "True:[1 0 0 1 0 1 1 1]\n",
            "27 + 124 = 119\n",
            "------------\n",
            "iters:2300\n",
            "Loss:0.6313059534895102\n",
            "Pred:[0 0 1 1 1 1 0 1]\n",
            "True:[0 0 1 1 1 0 0 1]\n",
            "2 + 55 = 61\n",
            "------------\n",
            "iters:2400\n",
            "Loss:0.478941397026562\n",
            "Pred:[0 1 1 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 0 0 0]\n",
            "100 + 4 = 96\n",
            "------------\n",
            "iters:2500\n",
            "Loss:0.42455133094752934\n",
            "Pred:[0 1 1 1 0 1 0 1]\n",
            "True:[0 1 1 1 0 1 0 1]\n",
            "116 + 1 = 117\n",
            "------------\n",
            "iters:2600\n",
            "Loss:0.4167595704853364\n",
            "Pred:[1 1 0 0 1 0 0 1]\n",
            "True:[1 1 0 0 1 0 0 1]\n",
            "102 + 99 = 201\n",
            "------------\n",
            "iters:2700\n",
            "Loss:0.46640649768158343\n",
            "Pred:[1 1 0 1 0 1 0 0]\n",
            "True:[1 1 0 1 0 1 0 0]\n",
            "124 + 88 = 212\n",
            "------------\n",
            "iters:2800\n",
            "Loss:0.20276103971414744\n",
            "Pred:[0 1 1 0 0 1 0 0]\n",
            "True:[0 1 1 0 0 1 0 0]\n",
            "84 + 16 = 100\n",
            "------------\n",
            "iters:2900\n",
            "Loss:0.48008898896123414\n",
            "Pred:[0 1 0 0 0 1 0 1]\n",
            "True:[0 1 0 0 1 1 0 1]\n",
            "52 + 25 = 69\n",
            "------------\n",
            "iters:3000\n",
            "Loss:0.8497825421170219\n",
            "Pred:[1 0 1 1 0 0 1 1]\n",
            "True:[1 1 0 0 0 0 1 1]\n",
            "78 + 117 = 179\n",
            "------------\n",
            "iters:3100\n",
            "Loss:0.5063097001425726\n",
            "Pred:[0 1 1 1 0 0 0 0]\n",
            "True:[0 1 1 1 0 1 0 0]\n",
            "94 + 22 = 112\n",
            "------------\n",
            "iters:3200\n",
            "Loss:0.3104150467946443\n",
            "Pred:[0 0 1 1 0 1 0 1]\n",
            "True:[0 0 1 1 0 1 0 1]\n",
            "39 + 14 = 53\n",
            "------------\n",
            "iters:3300\n",
            "Loss:0.5934408096914118\n",
            "Pred:[1 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 1 0 0]\n",
            "102 + 30 = 128\n",
            "------------\n",
            "iters:3400\n",
            "Loss:0.46651698425700966\n",
            "Pred:[0 1 1 1 1 1 1 0]\n",
            "True:[0 1 1 1 1 1 1 0]\n",
            "5 + 121 = 126\n",
            "------------\n",
            "iters:3500\n",
            "Loss:0.15563351288349497\n",
            "Pred:[1 0 0 1 1 1 0 1]\n",
            "True:[1 0 0 1 1 1 0 1]\n",
            "39 + 118 = 157\n",
            "------------\n",
            "iters:3600\n",
            "Loss:0.1866174091583235\n",
            "Pred:[1 0 0 0 1 0 1 0]\n",
            "True:[1 0 0 0 1 0 1 0]\n",
            "126 + 12 = 138\n",
            "------------\n",
            "iters:3700\n",
            "Loss:0.07327469185767439\n",
            "Pred:[0 1 0 0 1 0 1 0]\n",
            "True:[0 1 0 0 1 0 1 0]\n",
            "1 + 73 = 74\n",
            "------------\n",
            "iters:3800\n",
            "Loss:0.14925913683629632\n",
            "Pred:[0 1 1 0 1 1 0 0]\n",
            "True:[0 1 1 0 1 1 0 0]\n",
            "78 + 30 = 108\n",
            "------------\n",
            "iters:3900\n",
            "Loss:0.0370445877191648\n",
            "Pred:[1 0 1 0 0 0 0 1]\n",
            "True:[1 0 1 0 0 0 0 1]\n",
            "97 + 64 = 161\n",
            "------------\n",
            "iters:4000\n",
            "Loss:0.12249082202282535\n",
            "Pred:[0 1 1 1 1 0 0 1]\n",
            "True:[0 1 1 1 1 0 0 1]\n",
            "16 + 105 = 121\n",
            "------------\n",
            "iters:4100\n",
            "Loss:0.07617838238921137\n",
            "Pred:[1 1 0 1 0 0 0 1]\n",
            "True:[1 1 0 1 0 0 0 1]\n",
            "109 + 100 = 209\n",
            "------------\n",
            "iters:4200\n",
            "Loss:0.05460183047502394\n",
            "Pred:[0 1 0 0 1 1 0 1]\n",
            "True:[0 1 0 0 1 1 0 1]\n",
            "17 + 60 = 77\n",
            "------------\n",
            "iters:4300\n",
            "Loss:0.03592797323666887\n",
            "Pred:[0 1 0 1 1 1 0 0]\n",
            "True:[0 1 0 1 1 1 0 0]\n",
            "92 + 0 = 92\n",
            "------------\n",
            "iters:4400\n",
            "Loss:0.011759351173482747\n",
            "Pred:[0 1 0 1 1 0 1 0]\n",
            "True:[0 1 0 1 1 0 1 0]\n",
            "45 + 45 = 90\n",
            "------------\n",
            "iters:4500\n",
            "Loss:0.0968653819475821\n",
            "Pred:[1 0 0 0 1 1 1 1]\n",
            "True:[1 0 0 0 1 1 1 1]\n",
            "18 + 125 = 143\n",
            "------------\n",
            "iters:4600\n",
            "Loss:0.07541285402202193\n",
            "Pred:[1 1 0 0 0 0 0 1]\n",
            "True:[1 1 0 0 0 0 0 1]\n",
            "85 + 108 = 193\n",
            "------------\n",
            "iters:4700\n",
            "Loss:0.041866009702217974\n",
            "Pred:[0 1 0 1 0 0 0 1]\n",
            "True:[0 1 0 1 0 0 0 1]\n",
            "25 + 56 = 81\n",
            "------------\n",
            "iters:4800\n",
            "Loss:0.03793194164982545\n",
            "Pred:[1 0 0 0 0 1 1 1]\n",
            "True:[1 0 0 0 0 1 1 1]\n",
            "103 + 32 = 135\n",
            "------------\n",
            "iters:4900\n",
            "Loss:0.032914283301942016\n",
            "Pred:[1 0 1 1 1 0 1 1]\n",
            "True:[1 0 1 1 1 0 1 1]\n",
            "127 + 60 = 187\n",
            "------------\n",
            "iters:5000\n",
            "Loss:0.07113242886138459\n",
            "Pred:[1 1 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 0 0 0 0]\n",
            "101 + 91 = 192\n",
            "------------\n",
            "iters:5100\n",
            "Loss:0.03473649064625829\n",
            "Pred:[1 0 0 0 1 0 0 1]\n",
            "True:[1 0 0 0 1 0 0 1]\n",
            "29 + 108 = 137\n",
            "------------\n",
            "iters:5200\n",
            "Loss:0.0286213556009639\n",
            "Pred:[1 0 0 1 1 1 1 1]\n",
            "True:[1 0 0 1 1 1 1 1]\n",
            "41 + 118 = 159\n",
            "------------\n",
            "iters:5300\n",
            "Loss:0.029110871957960933\n",
            "Pred:[0 1 0 1 0 1 1 1]\n",
            "True:[0 1 0 1 0 1 1 1]\n",
            "38 + 49 = 87\n",
            "------------\n",
            "iters:5400\n",
            "Loss:0.025034633943676443\n",
            "Pred:[0 1 1 1 1 0 0 0]\n",
            "True:[0 1 1 1 1 0 0 0]\n",
            "65 + 55 = 120\n",
            "------------\n",
            "iters:5500\n",
            "Loss:0.043855762542858445\n",
            "Pred:[1 1 0 1 0 0 0 0]\n",
            "True:[1 1 0 1 0 0 0 0]\n",
            "85 + 123 = 208\n",
            "------------\n",
            "iters:5600\n",
            "Loss:0.02662038562829525\n",
            "Pred:[1 1 1 0 0 1 0 0]\n",
            "True:[1 1 1 0 0 1 0 0]\n",
            "119 + 109 = 228\n",
            "------------\n",
            "iters:5700\n",
            "Loss:0.017040498500216224\n",
            "Pred:[0 1 1 0 0 1 0 1]\n",
            "True:[0 1 1 0 0 1 0 1]\n",
            "69 + 32 = 101\n",
            "------------\n",
            "iters:5800\n",
            "Loss:0.022226039956537432\n",
            "Pred:[0 1 1 0 1 1 0 1]\n",
            "True:[0 1 1 0 1 1 0 1]\n",
            "91 + 18 = 109\n",
            "------------\n",
            "iters:5900\n",
            "Loss:0.027703511410743963\n",
            "Pred:[1 0 0 0 1 1 1 1]\n",
            "True:[1 0 0 0 1 1 1 1]\n",
            "76 + 67 = 143\n",
            "------------\n",
            "iters:6000\n",
            "Loss:0.0010971464538533512\n",
            "Pred:[0 0 0 1 0 1 1 0]\n",
            "True:[0 0 0 1 0 1 1 0]\n",
            "9 + 13 = 22\n",
            "------------\n",
            "iters:6100\n",
            "Loss:0.016660706306940432\n",
            "Pred:[1 0 1 1 1 1 0 1]\n",
            "True:[1 0 1 1 1 1 0 1]\n",
            "102 + 87 = 189\n",
            "------------\n",
            "iters:6200\n",
            "Loss:0.023644300080409415\n",
            "Pred:[1 0 1 0 0 1 0 0]\n",
            "True:[1 0 1 0 0 1 0 0]\n",
            "40 + 124 = 164\n",
            "------------\n",
            "iters:6300\n",
            "Loss:0.0015221713167509119\n",
            "Pred:[0 1 1 0 0 1 1 0]\n",
            "True:[0 1 1 0 0 1 1 0]\n",
            "35 + 67 = 102\n",
            "------------\n",
            "iters:6400\n",
            "Loss:0.008503117034273127\n",
            "Pred:[0 1 0 0 0 1 0 0]\n",
            "True:[0 1 0 0 0 1 0 0]\n",
            "51 + 17 = 68\n",
            "------------\n",
            "iters:6500\n",
            "Loss:0.016941591082515604\n",
            "Pred:[1 1 0 1 0 1 1 1]\n",
            "True:[1 1 0 1 0 1 1 1]\n",
            "89 + 126 = 215\n",
            "------------\n",
            "iters:6600\n",
            "Loss:0.020680829743199795\n",
            "Pred:[1 0 1 0 1 0 1 0]\n",
            "True:[1 0 1 0 1 0 1 0]\n",
            "108 + 62 = 170\n",
            "------------\n",
            "iters:6700\n",
            "Loss:0.009019119557070914\n",
            "Pred:[0 0 1 1 1 0 1 1]\n",
            "True:[0 0 1 1 1 0 1 1]\n",
            "41 + 18 = 59\n",
            "------------\n",
            "iters:6800\n",
            "Loss:0.009676876207828101\n",
            "Pred:[0 1 1 0 0 1 1 1]\n",
            "True:[0 1 1 0 0 1 1 1]\n",
            "71 + 32 = 103\n",
            "------------\n",
            "iters:6900\n",
            "Loss:0.013077197988792\n",
            "Pred:[0 1 1 0 0 0 0 1]\n",
            "True:[0 1 1 0 0 0 0 1]\n",
            "44 + 53 = 97\n",
            "------------\n",
            "iters:7000\n",
            "Loss:0.012122015503798373\n",
            "Pred:[1 0 0 0 1 0 0 1]\n",
            "True:[1 0 0 0 1 0 0 1]\n",
            "70 + 67 = 137\n",
            "------------\n",
            "iters:7100\n",
            "Loss:0.008836673893869704\n",
            "Pred:[1 0 0 1 1 0 0 1]\n",
            "True:[1 0 0 1 1 0 0 1]\n",
            "75 + 78 = 153\n",
            "------------\n",
            "iters:7200\n",
            "Loss:0.013382727185225388\n",
            "Pred:[0 1 0 0 0 0 1 0]\n",
            "True:[0 1 0 0 0 0 1 0]\n",
            "40 + 26 = 66\n",
            "------------\n",
            "iters:7300\n",
            "Loss:0.007836929921828675\n",
            "Pred:[0 1 1 1 0 0 1 1]\n",
            "True:[0 1 1 1 0 0 1 1]\n",
            "102 + 13 = 115\n",
            "------------\n",
            "iters:7400\n",
            "Loss:0.008133821678652543\n",
            "Pred:[0 1 1 0 1 0 0 0]\n",
            "True:[0 1 1 0 1 0 0 0]\n",
            "27 + 77 = 104\n",
            "------------\n",
            "iters:7500\n",
            "Loss:0.011614440541276774\n",
            "Pred:[0 1 1 0 1 1 0 0]\n",
            "True:[0 1 1 0 1 1 0 0]\n",
            "92 + 16 = 108\n",
            "------------\n",
            "iters:7600\n",
            "Loss:0.010592804493503864\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 1 1 1 1 1]\n",
            "116 + 11 = 127\n",
            "------------\n",
            "iters:7700\n",
            "Loss:0.007779178760048975\n",
            "Pred:[0 0 1 1 1 1 1 1]\n",
            "True:[0 0 1 1 1 1 1 1]\n",
            "9 + 54 = 63\n",
            "------------\n",
            "iters:7800\n",
            "Loss:0.010516266669159523\n",
            "Pred:[1 0 1 0 0 1 0 0]\n",
            "True:[1 0 1 0 0 1 0 0]\n",
            "72 + 92 = 164\n",
            "------------\n",
            "iters:7900\n",
            "Loss:0.008525692391507892\n",
            "Pred:[0 1 1 1 0 1 1 1]\n",
            "True:[0 1 1 1 0 1 1 1]\n",
            "0 + 119 = 119\n",
            "------------\n",
            "iters:8000\n",
            "Loss:0.006007162379721266\n",
            "Pred:[1 0 0 0 1 0 1 1]\n",
            "True:[1 0 0 0 1 0 1 1]\n",
            "37 + 102 = 139\n",
            "------------\n",
            "iters:8100\n",
            "Loss:0.008359833187277456\n",
            "Pred:[0 1 0 0 0 0 1 1]\n",
            "True:[0 1 0 0 0 0 1 1]\n",
            "36 + 31 = 67\n",
            "------------\n",
            "iters:8200\n",
            "Loss:0.00599145381038494\n",
            "Pred:[1 1 0 0 1 0 0 1]\n",
            "True:[1 1 0 0 1 0 0 1]\n",
            "95 + 106 = 201\n",
            "------------\n",
            "iters:8300\n",
            "Loss:0.000774891782172655\n",
            "Pred:[1 0 0 0 0 1 1 0]\n",
            "True:[1 0 0 0 0 1 1 0]\n",
            "101 + 33 = 134\n",
            "------------\n",
            "iters:8400\n",
            "Loss:0.001184405047331224\n",
            "Pred:[1 0 1 1 1 0 1 0]\n",
            "True:[1 0 1 1 1 0 1 0]\n",
            "99 + 87 = 186\n",
            "------------\n",
            "iters:8500\n",
            "Loss:0.006499711966681888\n",
            "Pred:[1 0 0 0 0 0 0 1]\n",
            "True:[1 0 0 0 0 0 0 1]\n",
            "20 + 109 = 129\n",
            "------------\n",
            "iters:8600\n",
            "Loss:0.004036691000676535\n",
            "Pred:[0 1 1 1 1 1 0 1]\n",
            "True:[0 1 1 1 1 1 0 1]\n",
            "90 + 35 = 125\n",
            "------------\n",
            "iters:8700\n",
            "Loss:0.007487941881073612\n",
            "Pred:[1 1 0 0 0 0 1 1]\n",
            "True:[1 1 0 0 0 0 1 1]\n",
            "96 + 99 = 195\n",
            "------------\n",
            "iters:8800\n",
            "Loss:0.0036524950675086067\n",
            "Pred:[0 0 1 0 0 0 0 1]\n",
            "True:[0 0 1 0 0 0 0 1]\n",
            "29 + 4 = 33\n",
            "------------\n",
            "iters:8900\n",
            "Loss:0.0035182146727848053\n",
            "Pred:[1 0 1 1 1 0 0 1]\n",
            "True:[1 0 1 1 1 0 0 1]\n",
            "82 + 103 = 185\n",
            "------------\n",
            "iters:9000\n",
            "Loss:0.008181694281748492\n",
            "Pred:[1 1 0 0 0 0 1 1]\n",
            "True:[1 1 0 0 0 0 1 1]\n",
            "100 + 95 = 195\n",
            "------------\n",
            "iters:9100\n",
            "Loss:0.0002818409476846181\n",
            "Pred:[0 1 0 1 1 1 1 0]\n",
            "True:[0 1 0 1 1 1 1 0]\n",
            "93 + 1 = 94\n",
            "------------\n",
            "iters:9200\n",
            "Loss:0.007049909700050715\n",
            "Pred:[1 0 0 1 1 1 1 0]\n",
            "True:[1 0 0 1 1 1 1 0]\n",
            "94 + 64 = 158\n",
            "------------\n",
            "iters:9300\n",
            "Loss:0.0065778692457498815\n",
            "Pred:[0 1 1 0 0 1 0 0]\n",
            "True:[0 1 1 0 0 1 0 0]\n",
            "40 + 60 = 100\n",
            "------------\n",
            "iters:9400\n",
            "Loss:0.0010980274373489916\n",
            "Pred:[1 0 1 1 0 0 1 0]\n",
            "True:[1 0 1 1 0 0 1 0]\n",
            "125 + 53 = 178\n",
            "------------\n",
            "iters:9500\n",
            "Loss:0.004327290672767455\n",
            "Pred:[0 1 1 1 0 1 1 1]\n",
            "True:[0 1 1 1 0 1 1 1]\n",
            "70 + 49 = 119\n",
            "------------\n",
            "iters:9600\n",
            "Loss:0.0011111253506623756\n",
            "Pred:[0 1 0 1 1 1 0 0]\n",
            "True:[0 1 0 1 1 1 0 0]\n",
            "21 + 71 = 92\n",
            "------------\n",
            "iters:9700\n",
            "Loss:0.000569426410369395\n",
            "Pred:[1 0 0 0 1 1 1 0]\n",
            "True:[1 0 0 0 1 1 1 0]\n",
            "107 + 35 = 142\n",
            "------------\n",
            "iters:9800\n",
            "Loss:0.005601271739648586\n",
            "Pred:[1 0 1 0 1 0 1 0]\n",
            "True:[1 0 1 0 1 0 1 0]\n",
            "114 + 56 = 170\n",
            "------------\n",
            "iters:9900\n",
            "Loss:0.0006064247459665305\n",
            "Pred:[0 0 1 0 0 1 1 0]\n",
            "True:[0 0 1 0 0 1 1 0]\n",
            "27 + 11 = 38\n",
            "------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsBt7vxmO-bn"
      },
      "source": [
        "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "## [try] weight_init_stdやlearning_rate, hidden_layer_sizeを変更してみよう\n",
        "\n",
        "\n",
        "## [try] 重みの初期化方法を変更してみよう\n",
        "Xavier, He\n",
        "\n",
        "## [try] 中間層の活性化関数を変更してみよう\n",
        "ReLU(勾配爆発を確認しよう)<br>\n",
        "tanh(numpyにtanhが用意されている。導関数をd_tanhとして作成しよう)\n",
        "\n",
        "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    }
  ]
}