<!DOCTYPE html>
<html>
<head>
<style>
body {
	background-color: white;
}
table {
  	border-collapse: collapse;
  	background-color: powderblue;
}
th, td {
  	border: 1px solid #333;
}
td {
  	text-align: center;
}
.header {
	padding: 10px;
	text-align: center;
	background:steelblue;
	color: black;
	font-size: 20px;
	border: 1px solid #333;

	overflow-y: auto; 
    position: sticky; 
	top: 0; 

}
.left { display: table-cell; text-align: left; }
</style>
</head>

<body>
	<div class="header">
		<h1>機械学習レポート</h1>
	</div>
	
	<p>	</p>

	<table>
		<tr>
			<th width="5%">ビデオ視聴学習者提出区分け</th>
			<th width="10%">科目</th>
			<th width="10%">章タイトル</th>
			<th width="30%" class="left">１点１００文字以上で要点のまとめ</th>
			<th width="10%" class="left">実装演習結果キャプチャーまたはサマリーと考察</th>
			<th width="10%" class="left">「確認テスト」など、自身の考察結果</th>
			<th width="10%" class="left">演習問題や参考図書、修了課題など関連記事レポートによる加点</th>
		</tr>
		<tr>
			<td rowspan="6">【b】</td>
			<td rowspan="6">機械学習</td>
			<td>
	線形回帰モデル
			</td>
			<td class="left">
				<ul>

	<li>機械学習の基本的な手法を理解し実装する。</li>

	<li>機械学習の定義（イアン・グッドフェローさんの本の中でのトム・ミッチェルさんによる定義）。</li>

	<li>線形回帰モデルの導入。</li>

	<li>例としての不動産価格の予測。</li>

	<li>線形結合。モデルのパラメータ。</li>

	<li>単回帰モデル（説明変数が１次元）直線、直線。重回帰モデル（説明変数が多次元）曲線。</li>

	<li>データの分割（学習用データと検証用でーた）とモデルの汎化性能。</li>

	<li>線形回帰モデルのパラメータは最小二乗法で推定。</li>

	<li>線形回帰の場合は、最尤法による解は最小二乗法による解と一致。</li>

	<li>平均二乗誤差（残差平方和）の微分は復習必要。（非線形、リッジ回帰（正則化）に関しても計算自体は全く一緒。）</li>

	<li>損失関数の選択（平均二乗誤差は一般に外れ値に弱い）。射影行列。</li>
	
				</ul>
			</td>
			<td>
				<a href="https://nbviewer.jupyter.org/github/road2021/RabbitChallenge/blob/main/2_1_1_1_skl_regression.ipynb" target="_blank">線形回帰モデル-Boston Housing Data-</a>
			</td>
			<td>
				不要
			</td>
			<td>
				<a href="https://nbviewer.jupyter.org/github/road2021/RabbitChallenge/blob/main/2_1_1_2_np_regression.ipynb" target="_blank">NumPyのみを使用した回帰モデル</a>
			</td>
		</tr>
		<tr>
			<td>
	非線形回帰モデル
			</td>
			<td class="left">
				<ul>

	<li>非線形回帰モデル。基底展開法。回帰関数として、基底関数と呼ばれる既知の非線形関数とパラメータベクトルとの線形結合を使用。</li>

	<li>基底関数には、多項式（１～９次）やガウス基底がある。基底展開法も線形回帰と同じ枠組みで推定可能。</li>

	<li>未学習（underfitting）と過学習（overfitting）。</li>

	<li>過学習への対策。不要な基底関数を削除。正則化法。</li>

	<li>罰則が無かった場合は、最小二乗推定量。L2ノルムを利用、Ridge推定量、縮小推定。L1ノルムを利用、Lasso推定量、スパース推定。</li>

	<li>モデルの表現力を押さえる（どの程度⇒ガンマ）基底関数の個数、位置、バンド巾そして正則化パラメータ。</li>

	<li>モデル選択。ホールドアウト法。クロスバリデーション（交差検証法）。あるモデルホールドアウトで検証したところ70％の精度、CVで65％だった場合でも、汎化性能の推定としてはCVを利用する。</li>

					</ul>
			</td>
			<td>
				<a href="https://nbviewer.jupyter.org/github/road2021/RabbitChallenge/blob/main/2_1_2_1_skl_nonlinear_regression.ipynb" target="_blank">非線形回帰モデル</a>
			</td>
			<td>
				不要				
			</td>
			<td>
			</td>
				</tr>
		<tr>
			<td>
	ロジスティック回帰モデル
			</td>
			<td class="left">
				<ul>
				</ul>
			</td>
			<td>
			</td>			
			<td>
				不要				
			</td>
			<td>
			</td>			
		</tr>
		<tr>
			<td>
	主成分分析
			</td>
			<td class="left">
				<ul>
					
				</ul>
			<td>
			</td>
			<td>
				不要				
			</td>			
			<td>
			</td>
		</tr>
		<tr>
			<td>
	アルゴリズム
			</td>
			<td class="left">
				<ul>
					
				</ul>
			<td>
			</td>
			<td>
				不要				
			</td>			
			<td>
			</td>
		</tr>	
		<tr>
			<td>
	サポートベクターマシーン
			</td>
			<td class="left">
				<ul>
					
				</ul>
			<td>
			</td>
			<td>
				不要				
			</td>			
			<td>
			</td>
		</tr>			
	</table>
</body>
</html>
