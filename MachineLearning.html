<!DOCTYPE html>
<html>
<head>

<meta name="twitter:title" content="機械学習レポート"/>
<meta name="twitter:description" content="機械学習の要点のまとめと実装演習結果へのリンクです。"/>
<meta name="twitter:image" content="https://ontheroad2021.github.io/RabbitChallenge/RabbitCallengeImage.png"/>
<meta name="twitter:card" content="summary"/>

<style>
body {
	background-color: white;
}
table {
  	border-collapse: collapse;
  	background-color: powderblue;
}
th, td {
  	border: 1px solid #333;
}
td {
  	text-align: center;
}
.header {
	padding: 10px;
	text-align: center;
	background:steelblue;
	color: black;
	font-size: 20px;
	border: 1px solid #333;

	overflow-y: auto; 
    position: sticky; 
	top: 0; 

}
.left { display: table-cell; text-align: left; }
</style>
</head>

<body>
	<div class="header">
		<h1>機械学習レポート</h1>
	</div>
	
	<p>	</p>

	<table>
		<tr>
			<th width="5%">ビデオ視聴学習者提出区分け</th>
			<th width="10%">科目</th>
			<th width="10%">章タイトル</th>
			<th width="30%" class="left">１点１００文字以上で要点のまとめ</th>
			<th width="10%" class="left">実装演習結果キャプチャーまたはサマリーと考察</th>
			<th width="10%" class="left">「確認テスト」など、自身の考察結果</th>
			<th width="10%" class="left">演習問題や参考図書、修了課題など関連記事レポートによる加点</th>
		</tr>
		<tr>
			<td rowspan="9">【b】</td>
			<td rowspan="9">機械学習</td>
			<td>
	線形回帰モデル
			</td>
			<td class="left">
				<ul>

	<li>機械学習の基本的な手法を理解し実装する。</li>

	<li>機械学習の定義（イアン・グッドフェローさんの本の中でのトム・ミッチェルさんによる定義）。</li>

	<li>線形回帰モデルの導入。</li>

	<li>例としての不動産価格の予測。</li>

	<li>線形結合。モデルのパラメータ。</li>

	<li>単回帰モデル（説明変数が１次元）直線、直線。重回帰モデル（説明変数が多次元）曲線。</li>

	<li>データの分割（学習用データと検証用でーた）とモデルの汎化性能。</li>

	<li>線形回帰モデルのパラメータは最小二乗法で推定。</li>

	<li>線形回帰の場合は、最尤法による解は最小二乗法による解と一致。</li>

	<li>平均二乗誤差（残差平方和）の微分は復習必要。（非線形、リッジ回帰（正則化）に関しても計算自体は全く一緒。）</li>

	<li>損失関数の選択（平均二乗誤差は一般に外れ値に弱い）。射影行列。</li>
	
				</ul>
			</td>
			<td>
				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/2_1_1_1_skl_regression.ipynb" target="_blank">線形回帰モデル（ボストンの住宅データ）</a>
			</td>
			<td>
				不要
			</td>
			<td>
				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/2_1_1_2_np_regression.ipynb" target="_blank">NumPyのみを使用した線形回帰モデル</a>
			</td>
		</tr>
		<tr>
			<td>
	非線形回帰モデル
			</td>
			<td class="left">
				<ul>

	<li>非線形回帰モデル。基底展開法。回帰関数として、基底関数と呼ばれる既知の非線形関数とパラメータベクトルとの線形結合を使用。</li>

	<li>基底関数には、多項式（１～９次）やガウス基底がある。基底展開法も線形回帰と同じ枠組みで推定可能。</li>

	<li>未学習（underfitting）と過学習（overfitting）。</li>

	<li>過学習への対策。不要な基底関数を削除。正則化法。</li>

	<li>罰則が無かった場合は、最小二乗推定量。L2ノルムを利用、Ridge推定量、縮小推定。L1ノルムを利用、Lasso推定量、スパース推定。</li>

	<li>モデルの表現力を押さえる（どの程度⇒ガンマ）基底関数の個数、位置、バンド巾そして正則化パラメータ。</li>

	<li>モデル選択。ホールドアウト法。クロスバリデーション（交差検証法）。あるモデルホールドアウトで検証したところ70％の精度、CVで65％だった場合でも、汎化性能の推定としてはCVを利用する。</li>

					</ul>
			</td>
			<td>
				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/2_1_2_1_skl_nonlinear_regression.ipynb" target="_blank">非線形回帰モデル</a>
			</td>
			<td>
				不要				
			</td>
			<td>
			</td>
				</tr>
		<tr>
			<td rowspan="3">
	ロジスティック回帰モデル
			</td>
			<td class="left">
				<ul>

	<li>「ロジスティック回帰モデル」</li>	
					
	<li>分類問題（クラス分類）例：実数全体を{0,1}へ移す。</li>

	<li>線形回帰で使用して線形結合をそのまま「シグモイド関数」の入力にしてしまう。（そのまま回帰モデルをあてはめると上手くいかない。）</li>

	<li>aを増加させる事により、曲線の勾配が増加し、単位ステップ関数に近づく。</li>

	<li>何故シグモイド関数を利用するのか？</li>

	<li>「良い性質」を持っている：シグモイド関数を微分した時に自分自身（シグモイド関数）で表現する事ができる。</li>

	<li>最適化の際に有利（MSEや尤度関数の最大、最小にする点を求める：微分が必要）</li>

	<li>シグモイド関数の出力をY＝１になる確率に対応させる。</li>

				</ul>
			</td>
			<td rowspan="3">
				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/2_1_3_1_skl_logistic_regression.ipynb" target="_blank">ロジスティック回帰モデル（タイタニックデータ）</a>
			</td>			
			<td rowspan="3">
				不要				
			</td>
			<td rowspan="3">
				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/2_1_3_2_np_logistic_regression.ipynb" target="_blank">NumPyのみを使用したロジスティック回帰モデル</a>
			</td>
		</tr>
		<tr>	
			<td class="left">
				<ul>

	<li>「最尤推定」</li>
					
	<li>ロジスティック回帰モデルではベルヌーイ分布を利用する。</li>
 
	<li>データからベルヌーイ分布のパラメータを推定。</li>

	<li>尤度関数を最大化するようなパラメータを選ぶ推定方法を最尤推定という。</li>

	<li>ロジスティック回帰モデルの最尤推定。</li>

	<li>確率ｐはシグモイド関数となるため、推定するパラメータは重みパラメータ（ｗ）となる。</li>

	<li>尤度関数はパラメータ（ｗ）のみに依存する関数。</li>

	<li>尤度関数を最大とするパラメータ（ｗ）を探す（推定）</li>

	<li>「マイナスをかけたもの（負の尤度関数）を最小化」し「最小二乗法の最小化」と合わせる。</li>

	<li>（対数をとると微分の計算が簡単）</li>

	<li>尤度とは確率ｐの掛け算。ｐは[0,1]の値を取るため、桁落ちの可能性大。</li>

	<li>勾配降下法（Gradient Descent）：解析的に解を求めるのは難しい。</li>

	<li>確率的勾配降下法（ＳＧＤ）：1回更新するたびに全部のデータをメモリにのせられない状況がある。ミニバッチ＝１。</li>

				</ul>
			</td>
		</tr>
		<tr>
			<td class="left">
				<ul>						

	<li>「モデルの評価」</li>

	<li>学習済みの「ロジスティック回帰モデル」の性能を測る。</li>
	
	<li>指標について。混同行列（Confusion Matrix）。</li>
	
	<li>分類の評価方法。</li>
	
	<li>「正解率」が良く使われる。</li>
	
	<li>再現率（Recall）。「癌である人に対して、あなたは癌ではありません」というのは病気の検診の場合だと絶対に避けなければいけません。</li>
	
	<li>適合率（Precision）。「スパムと予測したものが確実にスパムである」。</li>
	
	<li>Ｆ値。再現率と適合率の調和平均。</li>

				</ul>
			</td>			
		</tr>

		<tr>
			<td>
	主成分分析
			</td>
			<td class="left">
				<ul>

	<li>次元圧縮（多変量データの持つ構造をより小数個の指標に圧縮）。</li>
					
	<li>線形変換後の変数の分散（情報の量）が最大となる射影軸を探索。</li>
					
	<li>線形変換後の分散は分散共分散行列とａに関する２次形式で書ける（ａは係数ベクトル）。</li>
					
	<li>ラグランジュ関数を最大にする係数ベクトルを探索する事によって、制約つき最適化問題が解ける。</li>
					
	<li>主成分分析（ＰＣＡ）の結果、元のデータの分散共分散行列の固有値と固有ベクトルを求めれば、それが分散を最大にする軸になっている。</li>
					
	<li>寄与率：第Ｋ主成分が持つ情報量の割合。</li>
					
	<li>累積寄与率：第１-Ｋ主成分まで圧縮した際の情報損失量の割合。</li>	

				</ul>
			<td>
				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/2_1_4_1_skl_pca.ipynb" target="_blank">主成分分析（乳がんデータ）</a>
			</td>
			<td>
				不要				
			</td>			
			<td>
				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/2_1_4_2_np_pca.ipynb" target="_blank">NumPyのみを使用した主成分分析</a>
			</td>
		</tr>
		<tr>
			<td rowspan="2">
	アルゴリズム
			</td>
			<td class="left">
				<ul>

	<li>「ｋ近傍法（ｋＮＮ）」</li>

	<li>分類問題のための機械学習手法</li>

	<li>分類したい点の近傍からＫ個を取ってきて、それらがももっとも多く所属するクラスに識別。</li>

	<li>Ｋを大きくすると決定境界は滑らかになる。</li>	

				</ul>
			<td rowspan="2">
				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/2_1_5_2_skl_kmeans.ipynb" target="_blank">ｋ平均法（ワインの品質データ）</a>
			</td>
			<td rowspan="2">
				不要				
			</td>			
			<td>
				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/2_1_5_1_np_knn.ipynb" target="_blank">NumPyのみを使用したｋ近傍法（人口データ）</a>
			</td>
		</tr>	
		<tr>
			<td class="left">
				<ul>
					
	<li>「ｋ平均法（ｋ－ｍｅａｎｓ）」</li>					

				</ul>
			<td>			
		</tr>
		<tr>
			<td>
	サポートベクターマシーン
			</td>
			<td class="left">
				<ul>
					
				</ul>
			<td>
			</td>
			<td>
				不要				
			</td>			
			<td>
			</td>
		</tr>			
	</table>
</body>
</html>
