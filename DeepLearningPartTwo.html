<!DOCTYPE html>
<html>
<head>

<meta name="twitter:title" content="深層学習後半レポート"/>
<meta name="twitter:description" content="深層学習後半の要点のまとめと実装演習結果へのリンクです。"/>
<meta name="twitter:image" content="https://ontheroad2021.github.io/RabbitChallenge/images/RabbitChallengeImage.png"/>
<meta name="twitter:card" content="summary"/>

<style>
body {
	background-color: white;
}
table {
	border-collapse: collapse;
	background-color: lightgray;
}
th, td {
	border: 1px solid #333;
}
td {
	text-align: center;
}

.group { 
	background: powderblue;
	width: 100%;	
}

/*
.group:hover {
    background-color: lightsteelblue;
}
*/

.g1 { width:  5% }
.g2 { width: 10%; text-align: left; }
.g3 { width: 15%; text-align: left; }
.g4 { width: 25%; text-align: left; }
.g5 { width: 10%; text-align: }
.g6 { width: 10%; text-align: }
.g7 { width: 10%; text-align: }

.header {
	padding: 10px;
	text-align: center;
	background:steelblue;
	color: black;
	font-size: 20px;
	border: 1px solid #333;

	overflow-y: auto; 
    position: sticky; 
	top: 0; 
}
.left { display: table-cell; text-align: left; }
.right { display: table-cell; text-align: right; }

#navbar {
  padding: 0px;
  text-align: center;
  background-color: steelblue;
  color: black;
  font-size: 20px;
  /* border: 1px solid #333; */

  position: sticky; /* Make it stick/fixed */
  top: 0; /* Stay on top */
  transition: top 1.0s; /* Transition effect when sliding down (and up) */
}

</style>
</head>

<body>
	<div id="navbar">
		<table class="header">
			<tr>
				<th colspan="8"><h2>深層学習後半レポート</h2></th>
			</tr>
			<tr>
				<th width="5%">ビデオ視聴学習者提出区分け</th>
				<th width="10%">科目</th>
				<th width="15%">章タイトル</th>
				<th width="25%" class="left">１点１００文字以上で要点のまとめ</th>
				<th width="10%" class="left">実装演習結果キャプチャーまたはサマリーと考察</th>
				<th width="10%" class="left">「確認テスト」など、自身の考察結果</th>
				<th width="10%" class="left">演習問題や参考図書、修了課題など関連記事レポートによる加点</th>
			</tr>
		</table	>
	</div>
		
	<table class="group">
		<tr>
			<td class="g1" rowspan="13">【d】</td>
			<td class="g2" rowspan="7" class="left">深層学習day３</td>
			<td class="g3">
				Section１：<br>再帰型ニューラルネットワークの概念
			</td>
			<td class="g4">
				<ul>
					
<li>ＲＮＮ（Ｒｅｃｃｕｒｅｎｔ　Ｎｅｕｒａｌ　Ｎｅｔｏｗｒｋ)</li>

<li>時系列データ（音声データ、株価データ、テキストデータ）に対応可能なニューラルネットワーク</li>
					
<li>ＲＮＮの数学的記述</li>
					
<li>ＲＮＮの特徴（再帰的構造）</li>
					
<li>ＢＰＴＴ（Ｂａｃｋ　Ｐｒｏｐａｇａｔｉｏｎ　Ｔｈｒｏｕｇｈ　Ｔｉｍｅ）とは：ＲＮＮにおいてのパラメータ調整方法の一種</li>
					
<li>ＢＰＴＴの数学的記述</li>
					
<li>ｔ時間目とｔ－１時間目には関係がある事が見てとれる。</li>
					
<li>パラメータの更新式</li>
					
<li>ＲＮＮが過去の時間を再帰的にたくわえている事を示している。</li>

				</ul>
			</td>
			<td class="g5">
				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/4_1_1_1_3_1_simple_RNN.ipynb" target="_blank">ＲＮＮ</a><br>
			</td>
			<td class="g6">
				<a href="https://github.com/ontheroad2021/RabbitChallenge/blob/main/4_1_1_2_Review_Test.md" target="_blank">確認テスト（ＲＮＮ）</a>
			</td>
			<td class="g7">
				該当なし
			</td>
		</tr>
		<tr>
			<td class="left">
				Section２：ＬＳＴＭ
			</td>
			<td class="left">
				<ul>
<li>ＲＮＮの課題の解決：これまでに触れた勾配消失の解決方法（ｔａｎｈ等の勾配消失問題に強い活性化関数を用いる、あるいは正則化を用いる等）とは別に、構造自体を変えて解決したものがＬＳＴＭ。</li>

<li>勾配消失問題：誤差逆伝播法が下位層に進んで行くに連れて、勾配がどんどん緩やかになっていく。そのため、勾配降下法による更新では、下位層のパラメータはほとんど変わらず、訓練は最適解に収束しなくなる。（例）シグモイド関数</li>
										
<li>勾配爆発：勾配が層を逆伝播するごとに指数関数的に大きくなっていく⇒勾配クリッピング</li>
										
<li>ＬＳＴＭの全体図
					
					<ul>
					
<li>ＣＥＣ（Ｃｏｎｓｔａｎｔ Ｅｒｒｏｒ Ｃａｒｏｕｓｅｌ）記憶機能だけを持つもの</li>
										
<li>ＣＥＣの課題：入力データについて、時間依存に関係なく一律である（ニューラルネットワークの学習特性が無いという事）</li>
					
<li>入力ゲートと出力ゲートを追加する事で、それぞれのゲートへの入力値の重みを、重み行列で可変とする（ＣＥＣの課題を解決）</li>
					
					</ul>
									
<li>ＬＳＴＭの現状</li>

					<ul>
						
<li>ＣＥＣには過去の情報が全て保管されている。</li>
										
<li>ＬＳＴＭブロックの課題：過去の情報が要らなくなった場合、そのタイミングで情報を忘却する機能が必要⇒忘却ゲート</li>
										
<li>覗き穴結合：ＣＥＣ自身の値に重み行列を介して伝播可能にした構造（実際にはあまり大きな効果の改善は見られない。）</li>
										
					</ul>
				</ul>
			</td>
			<td>
				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/4_1_1_1_3_1_simple_RNN_after.ipynb" target="_blank">ＲＮＮ（アフター）</a>
			</td>
			<td>
				<a href="https://github.com/ontheroad2021/RabbitChallenge/blob/main/4_1_2_2_Review_Test.md" target="_blank">確認テスト（ＬＳＴＭ）</a>
			</td>
			<td>
				該当なし			
			</td>
		</tr>
		<tr>
			<td class="left">
				Section３：ＧＲＵ
			</td>
			<td class="left">
				<ul>

<li>ＧＲＵ（Ｇａｔｅｄ Ｒｅｃｕｒｒｅｎｔ Ｕｎｉｔ）</li>

<li>ＬＳＴＭの改良版：ＬＳＴＭでは、パラメータ数が多く、計算負荷が高くなる問題があった。</li>
					
<li>そのため、ＧＲＵはパラメータを大幅に削減し（計算負荷が低い）、精度は同等またはそれ以上が望める様になった構造を持つ。</li>

				</ul>
			</td>
			<td>
				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/4_1_2_1_predict_word.ipynb" target="_blank">ＧＲＵ</a>
			</td>
			<td>
				<a href="https://github.com/ontheroad2021/RabbitChallenge/blob/main/4_1_3_2_Review_Test.md" target="_blank">確認テスト（ＧＲＵ）</a>
			</td>
			<td>
				該当なし			
			</td>
		</tr>
		<tr>
			<td class="left">
				Section４：双方向ＲＮＮ
			</td>
			<td class="left">
				<ul>

<li>双方向ＲＮＮ（Ｂｉｄｉｒｅｃｔｉｏｎａｌ ＲＮＮ）</li>

<li>過去の情報だけでなく、未来の情報を加味する事で、精度を向上させるためのモデル。例）文章の推敲や機械翻訳等</li>
					
<li>文章というのは過去の情報も未来の情報も１度に入ってくる。過去の単語の羅列に加えて、その先に書いてある単語の羅列についても自由にアクセスができる。</li>
					
				</ul>
			</td>
			<td>
				該当なし
			</td>
			<td>
				該当なし
			</td>
			<td>
				該当なし		
			</td>
		</tr>
		<tr>
			<td class="left">
				Section５：Ｓｅｑ２Ｓｅｑ
			</td>
			<td class="left">
				

「Ｓｅｑ２ｓｅｑ（Ｓｅｑｕｅｎｃｅ to Ｓｅｑｕｅｎｃｅ）」

				<ul>

<li>Ｓｅｑ２ｓｅｑとはＥｎｃｏｄｅｒ－Ｄｅｃｏｄｅｒモデルの１種を指します（時系列データを入力に取って時系列のデータを出力するモデル）。</li>
					
<li>Ｓｅｑ２ｓｅｑの具体的な用途とは機械対話や機械翻訳等に使用されています。</li>
					
<li>Ｅｎｃｏｄｅｒ ＲＮＮ：文の意味を集約する。文から文脈の意味ベクトルの抽出。</li>
					
					<ul>
					
<li>Ｔａｋｉｎｇ：文章を単語等のトークン事に分割し、トークン毎のＩＤに分割する（ＯＮＥ ＨＯＴ ＶＥＣＴＯＲ）。</li>
					
<li>Ｅｍｂｅｄｄｉｎｇ：ＩＤからそのトークンを表す分散表現ベクトルに変換。数字の並びが似かよっている事は似かよった意味を持つ単語という事になる ⇒ 単語の意味を抽出したベクトル。</li>
					
<li>Ｅｎｃｏｄｅｒ ＲＮＮ：ベクトルを順番にＲＮＮに入力していく。</li>
					
					</ul>
						
<li>Ｅｎｃｏｄｅｒ ＲＮＮ処理手順</li>
						
					<ul>
						
<li>最後のベクトルを入れた時のｈｉｄｄｅｎ ｓｔａｔｅをｆｉｎａｌ ｓｔａｔｅとして取っておく。</li>
					
<li>このｆｉｎａｌ ｓｔａｔｅがｔｈｏｕｇｈｔ ｖｅｃｔｏｒと呼ばれ、入力した文の意味を表すベクトルになる。</li>
					
<li>ＭＬＭ － Ｍａｓｋｅｄ Ｌａｎｇｕａｇｅ Ｍｏｄｅｌ（ＢＥＲＴ）</li>
						
					</ul>
					
<li>Ｄｅｃｏｄｅｒ ＲＮＮ：システムがアウトプットデータを単語等のトークン毎に生成する構造。</li>
					
<li>Ｄｅｃｏｄｅｒ ＲＮＮ 処理手順</li>
					
					<ul>
						
<li>Ｄｅｃｏｄｅｒ ＲＮＮ：Ｅｎｃｏｄｅｒ ＲＮＮのｆｉｎａｌ ｓｔａｔｅ（ｔｈｏｕｇｈｔ ｖｅｃｔｏｒ）から、各ｔｏｋｅｎの生成確率を出力していきます。ｆｉｎａｌ ｓｔａｔｅをＤｅｃｏｄｅｒ ＲＮＮのｉｎｉｔｉａｌ ｓｔａｔｅとして設定し、Ｅｍｂｅｄｄｉｎｇを入力。</li>
					
<li>Ｓａｍｐｌｉｎｇ：生成確率に基づいてｔｏｋｅｎをランダムに選びます。</li>
					
<li>Ｅｍｂｅｄｄｉｎｇ：２で選ばれたｔｏｋｅｎをＥｍｂｅｄｄｉｎｇしてＤｅｃｏｄｅｒ ＲＮＮへの次の入力とします。</li>
					
<li>Ｄｅｔｏｋｅｎｉｚｅ：１～３を繰り返し、２で得られたｔｏｋｅｎを文字列に直します。</li>	
						
					</ul>
				</ul>
						
「ＨＲＥＤ(Ｔｈｅ Ｈｉｅｒａｒｃｈｉｃａｌ Ｒｅｃｕｒｒｅｎｔ Ｅｎｃｏｄｅｒ－Ｄｅｃｏｄｅｒ)」
				<ul>	
<li>Ｓｅｑ２ｓｅｑの課題：１問１答しかできない（問に対して文脈も何もなく、ただ応答が行われ続ける。）</li>
					
<li>ＨＲＥＤ：文章の過去の文脈というものを何かしら取れるようにできないかという試み。Ｓｅｑ２ｓｅｑが単語に扱ったのに加えて、文脈自体（１文、１文も）同様に扱ってみたら良いのではないか。</li>
					
<li>ＨＲＥＤとは：Ｓｅｑ２ｓｅｑ＋ＣｏｎｔｅｘｔＲＮＮ</li>
					
<li>ＣｏｎｔｅｘｔＲＮＮ：Ｅｎｃｏｄｅｒのまとめた各文章の系列をまとめて、これまでの会話コンテクスト全体を表すベクトルに変換する構造。 ⇒ 過去の発話の履歴を加味した返答ができる。</li>
					
<li>ＨＲＥＤの課題：（しくみを複雑にすると何か問題が起こるというのがディープラーニングの特徴です。）ＨＲＥＤでは確かに文脈を意識した文を作る事ができるのだが、ありがちな答えしか出さなくなる。あまりバリエーションに富んだ対話をしなくなってしまいます。例）「うん」「そうだね」等。</li>
					
				</ul>

「ＶＨＲＥＤ（Ｌａｔｅｎｔ Ｖａｒｉａｂｌｅ Ｈｉｅｒａｒｃｈｉｃａｌ Ｒｅｃｕｒｒｅｎｔ Ｅｎｃｏｄｅｒ－Ｄｅｃｏｄｅｒ）」
				<ul>	
<li>ＨＲＥＤというＳｅｑ２ｓｅｑとＣｏｎｔｅｘｔＲＮＮを合体させた文脈を意識しながら文を生成するモデルにヴァリエーションを持たせられるように工夫をしてみようという仕組みになります。</li>
				</ul>				
「オートエンコーダ」
				<ul>
<li>教師なし学習の１つ。次元削減を行う。（Ｅｎｃｏｄｅｒ、潜在変数Ｚ、Ｄｅｃｏｄｅｒ）からなる。</li>
				</ul>

「ＶＡＥ（Ｖａｒｉａｔｉｏｎａｌ Ａｕｔｏｅｎｃｏｄｅｒ）」

				<ul>
<li>洗剤変数Ｚを正規化する（確率分布Ｚ～Ｎ（０，１））。より汎用性の高い特徴を掴む。</li>						
				</ul>
			</td>
			<td>
				該当なし
			</td>
			<td>
				<a href="https://github.com/ontheroad2021/RabbitChallenge/blob/main/4_1_5_2_Review_Test.md" target="_blank">確認テスト（Ｓｅｑ２Ｓｅｑ）</a>
			</td>
			<td>
				該当なし			
			</td>
			<tr>
				<td class="left">
					Section６：Ｗｏｒｄ２ｖｅｃ
				</td>
				<td class="left">
					<ul>
						<li>単語のＯｎｅ－Ｈｏｔベクトルから１つ１うの単語（Ｅｍｂｅｄｄｉｎｇ表現）意味同志が近くなるようなベクトルを見つける。</li>
						<li>課題：ＲＮＮでは単語のような可変長の文字列をＮＮに与える事はできない ⇒ 固定長形式で単語を表す必要がある。</li>
						<li>メリット：大規模データの分散表現の学習が、現実的な計算速度とメモリ量で実現可能となった。Ｏｎｅ－ＨｏｔベクトルからＥｍｂｅｄｄｉｎｇ表現に変換する時に変換表を用いるが、この変換表を機械学習によって学習させる事で、単語の意味を保ちながら、より少ない次元のベクトルで単語を表現できる。</li>
							
	
					</ul>
				</td>
				<td>
					該当なし
				</td>
				<td>
					該当なし
				</td>
				<td>
					該当なし			
				</td>
				<tr>
					<td class="left">
						Section７：Ａｔｔｅｎｔｉｏｎ　Ｍｅｃｈａｎｉｓｍ
					</td>
					<td class="left">
						<ul>
							<li>課題：ｓｅｑ２ｓｅｑの問題は長い文章への対応が難しい事にある（ｓｅｑ２ｓｅｑの構造によると、長い文章であろうと短い文章であろうと、中間層（隠れ層）の大きさは決まっている。）ｓｅｑ２ｓｅｑでは２単語でも、１００単語でも、固定次元ベクトルの中に入力しなければならない。</li>
							<li>解決策：Ａｔｔｅｎｔｉｏｎ Ｍｅｃｈａｎｉｓｍ。１文の中で特に重要な単語というのを自力で見つけられるようにする機構。途中の中間層の情報量が一定でも重要な情報だけ拾い集める。</li>
							<li>近年特に性能が上がっている自然言語処理のモデルは全部Ａｔｔｅｎｔｉｏｎ Ｍｅｃｈａｎｉｓｍである程、強力である。</li>
						</ul>
							
							
							
							
					</td>
					<td>
						該当なし
					</td>
					<td>
						<a href="https://github.com/ontheroad2021/RabbitChallenge/blob/main/4_1_7_2_Review_Test.md" target="_blank">確認テスト（Ａｔｔｅｎｔｉｏｎ　Ｍｅｃｈａｎｉｓｍ）</a>
					</td>
					<td>
						該当なし			
					</td>
		</tr>
			<td rowspan="6" class="left">深層学習day４</td>
			<td class="left">
				Section１：強化学習
			</td>
			<td class="left">
				<ul>
<li>「強化学習」</li>						
					<ul>					
<li>教師あり、教師無し：特徴量を見つけ出し、未知のデータを予想する事が目標。</li>
<li>強化学習：優れた方策を見つける事が目標。</li>
					</ul>
<li>長期的に報酬を最大化できるように環境の中で行動を選択できるエージェントを作る事を目標とする機械学習の一分野 ⇒ 行動の結果として与えられる利益（報酬）をもとに行動を決定する原理を改善していく仕組みです。</li>
<li>探索と利用のトレードオフ：不完全な知識をもとに行動しながらデータを収集。最適な行動を見つけていく。</li>
						
<li>強化学習で学習するターゲット：方策関数、行動価値関数</li>
<li>強化学習の歴史</li>
					<ul>
<li>計算速度の進展により大規模な状態を持つ場合が可能となりつつある。</li>
<li>関数近似法（価値関数や方策関数を関数近似する手法の事）とＱ学習（行動価値関数を、行動する毎に更新する事により学習を進める方法）を組み合わせる手法の登場。</li>
					</ul>
<li>価値関数（価値関数、行動価値関数：状態と価値を組み合わせた価値に注目）</li>
<li>方策関数：エージェントが取る行動を決定するための関数。</li>
<li>方策勾配法について</li>
					<ul>
<li>方策反復法：方策をモデル化して最適化する手法 ⇒ 方策勾配法</li>
<li>定義方法：平均報酬、割引報酬和に対応して、行動価値関数の定義を行い、方策勾配定理が成立する。</li>
					</ul>
				</ul>
			</td>
			<td>
				不要
			</td>
			<td>
				不要	
			</td>
			<td>
				不要
			</td>
		</tr>	
		<tr>
			<td class="left">
				Section２：ＡｌｐｈａＧｏ
			</td>
			<td class="left">
				「Ａｌｐｈａ Ｇｏ Ｌｅｅ」
				<ul>
<li>Ａｌｐｈａ Ｇｏ ＬｅｅのＰｏｌｉｃｙＮｅｔ（方策関数）、ＶａｌｕｅＮｅｔ（価値関数）は共に畳み込みニューラルネットワーク。</li>
<li>Ｐｏｌｉｃｙ Ｎｅｔ（ＳｏｆｔＭａｘ Ｌａｙｅｒ）出力は１９ｘ１９マスの着手予想確率が出力される。</li>
<li>Ｖａｌｕｅ Ｎｅｔ（ＴａｎＨ Ｌａｙｅｒ）出力は現局面の勝率を-１～１で表したものが出力される。</li>
<li>Ａｌｐｈａ Ｇｏの学習</li>
					<ul>
<li>教師あり学習によるＲｏｌｌＯｕｔＰｏｌｉｃｙ（ＮＮではなく線形の方策関数。探索中に高速に着手確率を出すために使用される。ＰｏｌｉｃｙＮｅｔの１０００倍速い。）とＰｏｌｉｃｙＮｅｔの学習</li>
<li>強化学習によるＰｏｌｉｃｙＮｅｔの学習</li>
<li>強化学習によるＶａｌｕｅＮｅｔの学習</li>
					   </ul>
<li>ＰｏｌｉｃｙＮｅｔの強化学習</li>
<li>ＶａｌｕｅＮｅｔの学習</li>
<li>ＰｏｌｉｃｙＮｅｔとＲｏｌｌＯｕｔＰｏｌｉｃｙの教師あり学習。</li>
<li>モンテカルロ木探索（強化学習の学習方法）（価値関数を学習させる時に用いる方法。価値関数をどういうふうに更新するか。）</li>
				</ul>
				
		「Ａｌｐｈａ Ｇｏ ＬｅｅとＡｌｐｈａ Ｇｏ Ｚｅｒｏの違い」</li>
				<ul>
<li>教師あり学習を一切行わず、強化学習のみで作成。</li>
<li>特徴入力からヒューリスティックな要素を排除し、石の配置のみにした。</li>
<li>ＰｏｌｉｃｙＮｅｔとＶａｌｕｅＮｅｔを１つのネットワークに統合した。</li>
<li>Ｒｅｓｉｄｕａｌ Ｎｅｔ（後述）を導入した。</li>
<li>モンテカルロ木探索からＲｏｌｌＯｕｔシミュレーションをなくした。</li>
				</ul>
				
		「ＲｅｓｉｄｕａｌＮｅｔｗｏｒｋ」
				<ul>
<li>ネットワークにショートカット構造を追加して、勾配の爆発、消失を抑える効果を狙ったもの。</li>
<li>Residula Networkを使うことにより、100層を超えるネットワークでの安定した学習が可能となった。</li>
<li>基本構造はConvolution→BatchNorm→ReLU→Convolution→BatchNorm→Add→ReLUのBlockを１単位にして積み重ねる形となる。</li>
<li>また、Ｒｅｓｉｄｕａｌ Ｎｅｔｗｏｒｋを使うことにより層数の違うＮｅｔｗｏｒｋのアンサンブル効果が得られているという説もある。</li>
				</ul>							
			</td>
			<td>
				不要
			</td>
			<td>
				不要	
			</td>
			<td>
				不要			
			</td>
		</tr>
		<tr>
			<td class="left">
				Section３：軽量化・高速化技術
			</td>
			<td class="left">
				<ul>
<li>分散深層学習：データ並列、モデル並列</li>
<li>データ並列化：データを分割し、各ワーカー毎に計算させる。</li>
					<ul>
<li>同期型：同期型のパラメータ更新の流れ。各ワーカーが計算が終わるのを待ち、全ワーカーの勾配が出たところで勾配の平均を計算し、親モデルのパラメータを更新する。</li>
<li>非同期型：非同期型のパラメータ更新の流れ。各ワーカーはお互いの計算を待たず、各子モデルごとに更新を行う。学習が終わった子モデルはパラメータサーバにPushされる。新たに学習を始める時は、パラメータサーバからPopしたモデルに対して学習していく。</li>
					</ul>
<li>同期型と非同期型の比較</li>
					<ul>	
<li>処理のスピードは、お互いのワーカーの計算を待たない非同期型の方が早い。</li>
<li>非同期型は最新のモデルのパラメータを利用できないので、学習が不安定になりやすい。-> Stale Gradient Problem</li>
<li>現在は同期型の方が精度が良いことが多いので、主流となっている。</li>
					</ul>
<li>モデル並列</li>
					<ul>
<li>親モデルを各ワーカーに分割し、それぞれのモデルを学習させる。全てのデータで学習が終わった後で、一つのモデルに復元。</li>
<li>モデルが大きい時はモデル並列化を、データが大きい時はデータ並列化をすると良い。</li>	
<li>ある程度モデルが大きければ、分割した時の効果が目に見えるように出る。</li>
					</ul>		
<li>参照論文</li>
					<ul>
<li>Large Scale Distributed Deep Networks （Google社が2016年に出した論文。Tensorflowの前身といわれている。）</li>
<li>並列コンピューティングを用いることで大規模なネットワークを高速に学習させる仕組みを提案。</li>
<li>主にモデル並列とデータ並列(非同期型)の提案をしている。</li>	
					</ul>
<li>GPUによる高速化</li>
<li>モデルの軽量化まとめ</li>
					<ul>
<li>量子化（Quantization）：重みの精度を下げることにより計算の高速化と省メモリ化を行う技術。</li>
<li>蒸留（Distillaion）：複雑で精度の良い教師モデルから軽量な生徒モデルを効率よく学習を行う技術。</li>
<li>プルーニング：寄与の少ないニューロンをモデルから削減し高速化と省メモリ化を行う技術。</li>
					</ul>		
				</ul>		
			</td>
			<td>
				不要
			</td>
			<td>
				不要	
			</td>
			<td>
				不要			
			</td>
		</tr>
		<tr>
			<td class="left">
				Section４：応用モデル
			</td>
			<td class="left">
				「MobileNet（画像認識のモデルで軽量化したモデル）」
				<ul>
<li>２０１７年に精度は最高レベルに達しており、そこから先は軽くて性能が良いモデルを研究する事が多くなった。MobileNetはその先駆け。あまり計算量を増やさずに比較的高性能である事が目標。</li>
<li>一般的な畳み込みレイヤ：ストライド１でパディングを適用した場合の畳み込み計算の計算量。</li>
<li>MobileNetはここから、これをどうにか減らして行こうという試み。</li>
<li>Depthwise Convolution と Pointwise Convolution の組み合わせで軽量化を実現。</li>
					<ul>
					
<li>Depthwise Convolution：入力マップのチャネルごとに畳み込みを実施 ⇒ 計算量を大幅に削減可能。</li>
<li>Pointwise Convolution：1x1 Conv とも呼ばれる（正確には1x1xC） ⇒ 計算量を大幅に削減可能。</li>
					</ul>
				</ul>
			
			「DenseNet（画像認識のネットワーク）」
				<ul>
<li>DenseBlock</li>
					<ul>
<li>出力層に前の層の入力を足し合わせる。</li>
<li>DenseBlock内の各ブロック毎にk個ずつ特徴マップのチャネル数が増加していく時、kを成長率(Growth Rate)と呼ぶ。</li>
<li>ここでｋはハイパーパラメータである。</li>
<li>ｋが大きくなる程ネットワークが大きくなるため、小さな整数に設定するのが良い。</li>
					</ul>	
			
<li>Transition Layer</li>
					<ul>
<li>ＣＮＮでは中間層でチャネルサイズを変更する。</li>
<li>特徴マップのサイズを変更し、ダウンサンプリングを行うため、Transition Layerと呼ばれる層でDence blockをつなぐ。</li>
					</ul>
<li>以上のような構造になっている事で、各ブロック内で特徴マップのサイズは一致。</li>
<li>DenseNetとResNetの違い</li>
					<ul>
<li>DenseBlockでは前方の各層からの出力全てが後方の層への入力として用いられる。</li>
<li>RessidualBlockでは前1層の入力のみ後方の層へ入力。</li>
					</ul>
				</ul>
			
			「Batch Norm Layer」
				<ul>
<li>Batch Normの問題点</li>
<li>Batch Sizeが小さい条件下では、学習が収束しないことがあり、代わりにLayer Normalizationなどの正規化手法が使われることが多い。</li>
<li>Batch Sizeに影響を受ける。Batch Normalization というのは実際には使いたくない。ミニバッチというのは非常に厄介。学習を行う時のハードウェア（CPU, GPU, TPU）によって、ミニバッチサイズは変えざるを得ない。そうすると、Batch Normalization の効果というのはよく追えなくなる。</li>
				</ul>
			
			「Batch Norm以外の正規化」
				<ul>
<li>Batch Norm：ミニバッチに含まれるsampleの同一チャネルが同一分布に従うよう正規化</li>
<li>Layer Norm：それぞれのsampleの全てのpixelsが同一分布に従うよう正規化</li>
<li>Instance Nrom：さらにchannelも同一分布に従うよう正規化</li>
<li>ミニバッチのサイズの影響を受けるのはミニバッチ数で正規化を行っているものだけなため、Batch Norm のみがミニバッチのサイズの影響を受ける。</li>
				</ul>
				
			「Batch Norm」
				<ul>
<li>ミニバッチというのは演算器の性能によって、大きさというのはシビアに制限されてしまう。画像の認識モデルであれば、TPUは100枚くらいが限界、普通のCPUなら数枚から十数枚がバッチサイズ、ミニバッチサイズの上限となる。そのためBatch Normalization というのは使い勝手が良くない。</li>	
				</ul>
			
			「Layer Norm」
				<ul>
<li>直観的にはBatch NormとLayer Normはやっている事がだいぶ違う。これが、深層学習モデルの難しいところ。普通に考えるとBatch Normというのは正規化の方法としては順当である。ミニバッチの問題があったため、Layer Normのような手法が普通に用いられている。</li>
				</ul>
				
			「Instance Norm」
				<ul>
<li>各々の画像の中の１つ１つのチャネルの中だけで正規化をしてあげる。赤なら赤、緑なら緑、青なら青。このようにばらばらに各チャネルで正規化を行ったとしても、画像のスタイル転送というタスクであったり、テクスチャの合成というタスクでは比較的上手く行く。</li>
<li>正規化は、このようにニューラルネットワークを扱う上では、はっきりした要因はわからないけども上手くいくというのは昔からしられている内容である。正規化は、そもそもデータの特長をどれも同じようにそろえるという効果があるため直観的にはそれで上手く行くというのは分かるが、正規化の対象をいろいろ変えたとしても上手く育というところはなかなか難しいところになる。</li>	
				</ul>
			</td>
			<td>
				不要
			</td>
			<td>
				<a href="https://github.com/ontheroad2021/RabbitChallenge/blob/main/4_2_3_2_Review_Test.md" target="_blank">確認テスト（応用モデル）</a>	
			</td>
			<td>
				不要			
			</td>
		</tr>
		<tr>
			<td class="left">
				Section５：Ｔｒａｎｓｆｏｒｍｅｒ
			</td>
			<td class="left">
「Self-Attention(自己注意機構)」
				<ul>
<li>基本的にTransformerを構成している技術は全然新しい発明等では無く、過去のものを上手くくっつけて再構築したモデルになっているので、引用だらけの側面がある。計算量のわりに表現力の豊かなモデルとして知られています。</li>
<li>ニューラル機械翻訳の問題点：長さに弱い。
<li>上の問題意識から生まれた「Attention（注意機構）(2015)」
					<ul>
<li>情報量が多くなってきた時に何に注意を払って何に注意を払わないで良いかというのを学習的に決定していく、という機構が発明された。
					</ul>
<li>Attentionは何をしているのか
					<ul>
<li>Attentionは辞書オブジェクト：query(検索クエリ)に一致するKeyを索引し、対応するvalueを取り出す操作であると見做す事ができる。これは「辞書オブジェクト」の機能と同じである。（Key Value Attentionと呼ばれる。）
					</ul>
				</ul>
「Transformer(2017)」
					<ul>
<li>Attention is all you need</li>
<li>従来、Encoder-Decoder Model の基本だったRNNモデルを全く使用しない。
					</ul>	
「Transformer 主要モジュール」
					<ul>
<li>RNNを使用していないので文字の情報を保存できない ⇒ 単語ベクトルに単語の位置を付加。
<li>大きくEncoderとDecoderに分かれていて、どちらもSelf-Attentionを使用しています。（Attentionと言われていても、Transformerで使用されているものと、もともとAttentionが発明された時に使用されていたものは別ものである。ソース・ターゲット注意機構。自己注意機構。）
<li>Self-Attentionのイメージとしては、CNNが１番良い。文脈を反映した自己表現を得られる。CNNはあるピクセルを中心にした時に、その周囲の情報をConvolutionして抽象化した情報が出て来る。Attentionもそれに近い事をしている。Self-AttentionとCNNの対応関係はよく言われている事である。数式は違うが、どちらも文脈依存のConvolutionである。２つの違いは、CNNはあくまでウィンドウサイズがあるので、決められた範囲のConvolutionしかおこなわないが、Self-AttentionはInputされた全ての情報からそれぞれを定義していくので、ウィンドウサイズが文の長さのConvolutionと考えられる。
					</ul>	
「Transformer-Encoder」
					<ul>
<li>系列をインプットして、その系列が位置情報を失わないままSelf-Attention層を流れて行き、内部状態に変換される。
					</ul>	
「Feed-Forward Networks」
					<ul>
<li>全結合層（位置情報を保持したまま順伝播させる）
<li>使用目的としては線形変換をかける層で、最終的にアウトプットの次元をそろえないといけないのでマトリックスを整えるための全結合層。
					</ul>	
「Scaled Dot-Product Attention」
					<ul>
<li>前単語に関するAttentionをまとめて計算する（Self-Attentionのミニマムな構造）
					</ul>
「Multi-Head Attention」
					<ul>
<li>上を連結したもの。８つにUNITを分けている意図はAttentionのメカニズムを分離させる事で、それぞれのAttention層が、それぞれの独自の注意のかけ方を学習し、総合的により良い注意のかけ方になるのが狙い。実際、内部のAttentionを可視化してみても、それぞれのAttention層ごとに全然違うAttentionのかけ方をしていて、役割を分担させるアンサンブル学習のような働きをしている。
					</ul>			
「Decoder」
					<ul>
<li>Encoder側と同様なMulti-Head Attentionが２個かかっている。（Self-Attention(下)とSame-Target Attention(上)が使用されている。）
					</ul>	
「Position Encoding」
					<ul>
<li>RNNを用いないので単語別の語順情報を追加する必要がある。
					</ul>
				</ul>						
			</td>
			<td>
				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/4_2_5_1_lecture_chap1_exercise_public.ipynb" target="_blank">Ｓｅｑ２ｓｅｑ</a><br>
				<a href="https://nbviewer.jupyter.org/github/ontheroad2021/RabbitChallenge/blob/main/4_2_5_1_lecture_chap2_exercise_public.ipynb" target="_blank">Ｔｒａｎｓｆｏｒｍｅｒ</a><br>

			</td>
			<td>
				不要	
			</td>
			<td>
				不要			
			</td>
		</tr>
		<tr>
			<td class="left">
				Section６：物体検知・セグメンテーション
			</td>
			<td class="left">
「物体認識タスク」
				<ul>
<li>分類：（画像に対し単一または複数の）クラスラベル
<li>物体検知：Bounding Box
<li>セマンティックセグメンテーション（意味領域分割）：（各ピクセルに対し単一の） クラスラベル
<li>インスタンスセグメンテーション（個体領域分割）：（各ピクセルに対し単一の） クラスラベル
				</ul>
				
「代表的なデータセット」
				<ul>
<li>クラス数が大きい事はうれしいのか？
<li>目的に応じた「BOX/画像」の選択を。
					<ul>
<li>小：アイコン的な映り。日常感とはかけ離れやすい。
<li>大：部分的な重なり等も見られる。日常生活のコンテクストに近い。
					</ul>
<li>目的に合ったデータセットを選ぶ事が大切（クラス数あるいはBOX／画像を１つの基準として）
					<ul>
<li>VOC12（Visual Object Classes）
<li>ILSVRC12: ImageNetのサブセット
<li>MS COCO18 (Microsoft Common Object in Context)
<li>OICOD18 (Open Images Challenges Object Detection)
					</ul>
				</ul>
			</ul>
「評価指標」
			<ul>
<li>分類問題における評価指標の復習
				<ul>
<li>Confusion Matrix
					<ul>
<li>Precision = TP/(TP + FP)
<li>Recall = TP/(TP + FN)
					</ul>
<li>Conf.の閾値を変化させる事でPrecision-Recall curveが描ける。
<li>クラス分類の場合、閾値を変えても混同行列に含まれる件数は変わらない。
<li>物体検出の場合はクラス分類と異なり、件数が変わりうる。
				</ul>
<li>IoU（Intersection over Union）
				<ul>
<li>物体検出においてはクラスラベルだけでなく, 物体位置の予測精度も評価したい。
<li>IoU = Area of Overlap / Area of Union
<li>IoUは値の直観的解釈が難しい。
				</ul>
<li>（入力１枚で見る）Precision/Recall
<li>Precision/Recallの計算例（クラス単位）
				<ul>
<li>このPrecisionやRecallはConf.の閾値を変えれば変わるはずでした。
<li>そこから出て来るのが物体検知で用いられる指標：Average Precision (AP)
				</ul>	
<li>AP: Average Precision
				<ul>
<li>PR曲線を描くためにConf.を0.05から変化させていくと、conf.の閾値の変化に伴ってRecall, Precision が変わり、それによってPR曲線が描ける。
<li>物体検出の精度評価の指標：AP = PR曲線の下側面積
<li>APが大きい方が良い精度指標になっている。
				</ul>	
<li>mAP: mean Average Precision
				<ul>	
<li>APの平均（APはクラスごとに計算される。）
				</ul>	
<li>ｍAP COCO: MS COCO で導入された指標
				<ul>
<li>ここまで, IoU閾値は0.5で固定 → 0.5から0.95まで0.05刻みでAP＆mAPを計算し算術平均を計算
				</ul>	
<li>FPS：Flames per Second
				<ul>
 <li>物体検知応用上の要請から, 検出精度に加え検出速度も問題となる。
				</ul>	
			</ul>	
「物体検知の大枠」
				<ul>
<li>マイルストーン：深層学習以降の物体検知
				<ul>
<li>2012 AlexNetの登場を皮切りに, 時代はSIFTからDCNNへ 
				</ul>
<li>物体検知のフレームワーク
				<ul>
<li>１段階検出器（One-stage detector）
					<ul>
<li>候補領域の検出とクラス推定を同時に行う 
<li>相対的に精度が低い傾向 
<li>相対的に計算量が小さく推論も早い傾向
					</ul>	
<li>２段階検出器（Two-stage detector）
					<ul>
<li>候補領域の検出とクラス推定を別々に行う 
<li>相対的に精度が高い傾向
<li>相対的に計算量が大きく推論も遅い傾向
					</ul>
				</ul>							
			</td>
			<td>
				不要
			</td>
			<td>
				不要	
			</td>
			<td>
				不要			
			</td>
		</tr>	
	</table>

	<script>
		/* When the user scrolls down, hide the navbar. When the user scrolls up, show the navbar */
		var prevScrollpos = window.pageYOffset;
		window.onscroll = function() {
		var currentScrollPos = window.pageYOffset;
		  if (prevScrollpos > currentScrollPos) {
			document.getElementById("navbar").style.top = "0";
		  } else {
			document.getElementById("navbar").style.top = "-250px";
		  }
		  prevScrollpos = currentScrollPos;
		}
	</script>

</body>
</html>
