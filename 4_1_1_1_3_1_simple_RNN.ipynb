{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "3_1_simple_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cNl2QA_Rnv5"
      },
      "source": [
        "# 準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkwjN1jNVAYy"
      },
      "source": [
        "## Googleドライブのマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvFXpiH3EVC1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22e4aff9-b7c9-494d-dd01-bf7132450006"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ub7RYdeY6pK"
      },
      "source": [
        "## sys.pathの設定"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oql7L19rEsWi"
      },
      "source": [
        "以下では，Googleドライブのマイドライブ直下にDNN_codeフォルダを置くことを仮定しています．必要に応じて，パスを変更してください．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ic2JzkvFX59"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/DNN_code')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feXB1SiLP4OL"
      },
      "source": [
        "# simple RNN\n",
        "### バイナリ加算"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKmDeLS8qN_2"
      },
      "source": [
        "中間層の活性化関数がsigmoidの時、学習は比較的上手くいく。XavierやHeの初期値を使用する事によって収束により時間がかかるように思われる。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PU8SW8RFPLRM",
        "outputId": "f7738df2-99b2-4cc2-d3b7-078a9c20f05e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "from common import functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def d_tanh(x):\n",
        "  sinh = (np.exp(x) - np.exp(-x)) / 2\n",
        "  cosh = (np.exp(x) + np.exp(-x)) / 2\n",
        "  return (cosh**2 - sinh**2) / cosh**2\n",
        "\n",
        "# データを用意\n",
        "# 2進数の桁数\n",
        "binary_dim = 8\n",
        "# 最大値 + 1\n",
        "largest_number = pow(2, binary_dim)\n",
        "# largest_numberまで2進数を用意\n",
        "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
        "\n",
        "input_layer_size = 2\n",
        "hidden_layer_size = 16 #16\n",
        "output_layer_size = 1\n",
        "\n",
        "weight_init_std = 1\n",
        "learning_rate = 0.1 #0.1\n",
        "\n",
        "iters_num = 10000\n",
        "plot_interval = 100\n",
        "\n",
        "# ウェイト初期化 (バイアスは簡単のため省略)\n",
        "W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
        "W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
        "W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n",
        "\n",
        "# Xavier\n",
        "W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size))\n",
        "W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "\n",
        "# He\n",
        "# W_in = np.random.randn(input_layer_size, hidden_layer_size) / np.sqrt(input_layer_size) * np.sqrt(2)\n",
        "# W_out = np.random.randn(hidden_layer_size, output_layer_size) / np.sqrt(hidden_layer_size) * np.sqrt(2)\n",
        "# W = np.random.randn(hidden_layer_size, hidden_layer_size) / np.sqrt(hidden_layer_size) * np.sqrt(2)\n",
        "\n",
        "# 勾配\n",
        "W_in_grad = np.zeros_like(W_in)\n",
        "W_out_grad = np.zeros_like(W_out)\n",
        "W_grad = np.zeros_like(W)\n",
        "\n",
        "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "y = np.zeros((output_layer_size, binary_dim))\n",
        "\n",
        "delta_out = np.zeros((output_layer_size, binary_dim))\n",
        "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "\n",
        "all_losses = []\n",
        "\n",
        "for i in range(iters_num):\n",
        "    \n",
        "    # A, B初期化 (a + b = d)\n",
        "    a_int = np.random.randint(largest_number/2)\n",
        "    a_bin = binary[a_int] # binary encoding\n",
        "    b_int = np.random.randint(largest_number/2)\n",
        "    b_bin = binary[b_int] # binary encoding\n",
        "    \n",
        "    # 正解データ\n",
        "    d_int = a_int + b_int\n",
        "    d_bin = binary[d_int]\n",
        "    \n",
        "    # 出力バイナリ\n",
        "    out_bin = np.zeros_like(d_bin)\n",
        "    \n",
        "    # 時系列全体の誤差\n",
        "    all_loss = 0    \n",
        "    \n",
        "    # 時系列ループ\n",
        "    for t in range(binary_dim):\n",
        "        # 入力値\n",
        "        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n",
        "        # 時刻tにおける正解データ\n",
        "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
        "        \n",
        "        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
        "        z[:,t+1] = functions.sigmoid(u[:,t+1])\n",
        "\n",
        "        y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "\n",
        "\n",
        "        #誤差\n",
        "        loss = functions.mean_squared_error(dd, y[:,t])\n",
        "        \n",
        "        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_sigmoid(y[:,t])        \n",
        "        \n",
        "        all_loss += loss\n",
        "\n",
        "        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n",
        "    \n",
        "    \n",
        "    for t in range(binary_dim)[::-1]:\n",
        "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
        "\n",
        "        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])\n",
        "\n",
        "        # 勾配更新\n",
        "        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n",
        "        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n",
        "        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n",
        "    \n",
        "    # 勾配適用\n",
        "    W_in -= learning_rate * W_in_grad\n",
        "    W_out -= learning_rate * W_out_grad\n",
        "    W -= learning_rate * W_grad\n",
        "    \n",
        "    W_in_grad *= 0\n",
        "    W_out_grad *= 0\n",
        "    W_grad *= 0\n",
        "    \n",
        "\n",
        "    if(i % plot_interval == 0):\n",
        "        all_losses.append(all_loss)        \n",
        "        print(\"iters:\" + str(i))\n",
        "        print(\"Loss:\" + str(all_loss))\n",
        "        print(\"Pred:\" + str(out_bin))\n",
        "        print(\"True:\" + str(d_bin))\n",
        "        out_int = 0\n",
        "        for index,x in enumerate(reversed(out_bin)):\n",
        "            out_int += x * pow(2, index)\n",
        "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n",
        "        print(\"------------\")\n",
        "\n",
        "lists = range(0, iters_num, plot_interval)\n",
        "plt.plot(lists, all_losses, label=\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iters:0\n",
            "Loss:0.9429043122747464\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 0 0 1]\n",
            "24 + 57 = 0\n",
            "------------\n",
            "iters:100\n",
            "Loss:1.0547342079647437\n",
            "Pred:[0 0 1 1 0 0 1 0]\n",
            "True:[1 0 0 0 0 1 1 1]\n",
            "77 + 58 = 50\n",
            "------------\n",
            "iters:200\n",
            "Loss:0.9232689074956549\n",
            "Pred:[1 0 0 1 1 0 1 1]\n",
            "True:[1 0 0 0 1 1 1 1]\n",
            "116 + 27 = 155\n",
            "------------\n",
            "iters:300\n",
            "Loss:1.0478613808689874\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[1 0 0 0 0 0 0 1]\n",
            "61 + 68 = 255\n",
            "------------\n",
            "iters:400\n",
            "Loss:1.0290011776281351\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 1 1 1 0]\n",
            "123 + 35 = 0\n",
            "------------\n",
            "iters:500\n",
            "Loss:0.9550435162731711\n",
            "Pred:[0 0 0 0 0 1 1 1]\n",
            "True:[0 0 0 0 1 1 1 1]\n",
            "14 + 1 = 7\n",
            "------------\n",
            "iters:600\n",
            "Loss:0.9692198407121941\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[0 0 1 0 1 1 0 1]\n",
            "32 + 13 = 255\n",
            "------------\n",
            "iters:700\n",
            "Loss:1.1067852419472106\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 1 1 1 1]\n",
            "60 + 115 = 0\n",
            "------------\n",
            "iters:800\n",
            "Loss:1.0298509401071865\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[1 1 1 1 0 0 0 0]\n",
            "115 + 125 = 255\n",
            "------------\n",
            "iters:900\n",
            "Loss:0.9692590446114921\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[1 0 1 1 0 1 1 0]\n",
            "106 + 76 = 255\n",
            "------------\n",
            "iters:1000\n",
            "Loss:1.055321675868427\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[1 1 1 0 0 0 0 0]\n",
            "111 + 113 = 255\n",
            "------------\n",
            "iters:1100\n",
            "Loss:0.927986193282049\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 0 1 1 0 1]\n",
            "60 + 49 = 255\n",
            "------------\n",
            "iters:1200\n",
            "Loss:1.0170836525473521\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 1 1 0]\n",
            "62 + 56 = 0\n",
            "------------\n",
            "iters:1300\n",
            "Loss:1.1555699065883815\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[1 0 0 0 1 0 0 0]\n",
            "90 + 46 = 255\n",
            "------------\n",
            "iters:1400\n",
            "Loss:1.028184219570866\n",
            "Pred:[1 1 1 1 0 1 1 0]\n",
            "True:[1 0 0 0 0 0 1 1]\n",
            "25 + 106 = 246\n",
            "------------\n",
            "iters:1500\n",
            "Loss:1.0345363268812473\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 1 1 0 1]\n",
            "110 + 31 = 0\n",
            "------------\n",
            "iters:1600\n",
            "Loss:1.0053243487348487\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 1 1 0]\n",
            "85 + 1 = 0\n",
            "------------\n",
            "iters:1700\n",
            "Loss:1.0617547928677646\n",
            "Pred:[1 1 1 1 1 1 1 0]\n",
            "True:[1 0 1 0 1 0 0 0]\n",
            "86 + 82 = 254\n",
            "------------\n",
            "iters:1800\n",
            "Loss:0.9367283328147183\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 1 0 0]\n",
            "60 + 40 = 0\n",
            "------------\n",
            "iters:1900\n",
            "Loss:0.9529932891870629\n",
            "Pred:[0 0 0 1 1 1 1 0]\n",
            "True:[0 1 0 1 1 1 1 1]\n",
            "52 + 43 = 30\n",
            "------------\n",
            "iters:2000\n",
            "Loss:1.0355187862422457\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 1 0 0 1 0 0]\n",
            "126 + 102 = 0\n",
            "------------\n",
            "iters:2100\n",
            "Loss:0.9687566606277976\n",
            "Pred:[0 0 1 1 1 1 1 1]\n",
            "True:[0 0 1 1 1 0 0 1]\n",
            "30 + 27 = 63\n",
            "------------\n",
            "iters:2200\n",
            "Loss:0.9704636216180795\n",
            "Pred:[1 1 1 1 1 1 0 0]\n",
            "True:[1 1 1 0 1 1 0 0]\n",
            "110 + 126 = 252\n",
            "------------\n",
            "iters:2300\n",
            "Loss:0.9869748464208119\n",
            "Pred:[1 1 1 1 0 0 1 0]\n",
            "True:[1 0 0 0 0 0 1 0]\n",
            "112 + 18 = 242\n",
            "------------\n",
            "iters:2400\n",
            "Loss:0.954096876172498\n",
            "Pred:[0 0 1 1 0 1 1 0]\n",
            "True:[0 0 1 1 1 0 0 0]\n",
            "2 + 54 = 54\n",
            "------------\n",
            "iters:2500\n",
            "Loss:0.9601428749231653\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 1 1 0 1]\n",
            "75 + 2 = 0\n",
            "------------\n",
            "iters:2600\n",
            "Loss:1.0756203665565085\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 0 0 0 0 0]\n",
            "55 + 41 = 127\n",
            "------------\n",
            "iters:2700\n",
            "Loss:0.8922590014762107\n",
            "Pred:[0 1 0 1 0 1 1 0]\n",
            "True:[0 1 0 1 0 1 1 0]\n",
            "68 + 18 = 86\n",
            "------------\n",
            "iters:2800\n",
            "Loss:1.1676772155578494\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[1 0 0 0 1 1 0 0]\n",
            "79 + 61 = 127\n",
            "------------\n",
            "iters:2900\n",
            "Loss:1.002912500578577\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[1 0 1 0 0 0 1 1]\n",
            "91 + 72 = 255\n",
            "------------\n",
            "iters:3000\n",
            "Loss:0.7987469157001726\n",
            "Pred:[0 1 0 1 0 1 0 0]\n",
            "True:[0 1 0 1 0 1 1 0]\n",
            "52 + 34 = 84\n",
            "------------\n",
            "iters:3100\n",
            "Loss:0.7149268851419333\n",
            "Pred:[0 1 0 1 1 1 1 1]\n",
            "True:[0 1 0 0 1 1 1 1]\n",
            "2 + 77 = 95\n",
            "------------\n",
            "iters:3200\n",
            "Loss:0.7718075392778798\n",
            "Pred:[0 1 1 1 0 1 1 1]\n",
            "True:[0 1 0 1 0 1 0 1]\n",
            "35 + 50 = 119\n",
            "------------\n",
            "iters:3300\n",
            "Loss:0.8284222357928411\n",
            "Pred:[0 0 0 0 1 1 0 0]\n",
            "True:[0 0 1 0 0 0 0 1]\n",
            "23 + 10 = 12\n",
            "------------\n",
            "iters:3400\n",
            "Loss:1.034203930090837\n",
            "Pred:[0 1 1 1 0 1 1 1]\n",
            "True:[1 0 0 1 0 1 1 1]\n",
            "31 + 120 = 119\n",
            "------------\n",
            "iters:3500\n",
            "Loss:1.1980166328116486\n",
            "Pred:[0 1 1 0 1 0 0 1]\n",
            "True:[1 0 0 1 0 0 0 1]\n",
            "29 + 116 = 105\n",
            "------------\n",
            "iters:3600\n",
            "Loss:0.8364680030693465\n",
            "Pred:[0 0 0 1 1 0 1 1]\n",
            "True:[0 1 0 1 0 0 1 1]\n",
            "14 + 69 = 27\n",
            "------------\n",
            "iters:3700\n",
            "Loss:0.741856631826524\n",
            "Pred:[0 1 1 1 1 0 1 1]\n",
            "True:[0 1 1 1 1 0 1 1]\n",
            "104 + 19 = 123\n",
            "------------\n",
            "iters:3800\n",
            "Loss:0.6908321594921079\n",
            "Pred:[1 1 1 0 0 0 1 1]\n",
            "True:[0 1 1 1 0 0 1 1]\n",
            "64 + 51 = 227\n",
            "------------\n",
            "iters:3900\n",
            "Loss:1.4031953652613263\n",
            "Pred:[0 1 1 0 0 0 1 0]\n",
            "True:[1 0 0 1 1 1 0 0]\n",
            "29 + 127 = 98\n",
            "------------\n",
            "iters:4000\n",
            "Loss:0.6595996408494089\n",
            "Pred:[0 1 1 0 0 1 0 0]\n",
            "True:[0 1 1 0 0 1 1 0]\n",
            "22 + 80 = 100\n",
            "------------\n",
            "iters:4100\n",
            "Loss:0.7000554527932431\n",
            "Pred:[0 1 1 1 1 0 0 1]\n",
            "True:[0 1 1 1 1 1 0 1]\n",
            "103 + 22 = 121\n",
            "------------\n",
            "iters:4200\n",
            "Loss:0.7902416693194086\n",
            "Pred:[1 0 1 1 0 1 0 1]\n",
            "True:[1 0 1 0 0 1 0 1]\n",
            "88 + 77 = 181\n",
            "------------\n",
            "iters:4300\n",
            "Loss:0.7221798898007857\n",
            "Pred:[1 0 1 0 1 1 0 0]\n",
            "True:[1 1 0 0 1 1 0 0]\n",
            "82 + 122 = 172\n",
            "------------\n",
            "iters:4400\n",
            "Loss:1.1010988244467348\n",
            "Pred:[1 0 0 0 0 0 1 0]\n",
            "True:[1 1 1 1 1 0 1 0]\n",
            "124 + 126 = 130\n",
            "------------\n",
            "iters:4500\n",
            "Loss:0.5102437508231092\n",
            "Pred:[1 0 0 1 0 1 0 1]\n",
            "True:[1 0 0 1 0 1 0 1]\n",
            "69 + 80 = 149\n",
            "------------\n",
            "iters:4600\n",
            "Loss:0.45335468034822524\n",
            "Pred:[0 1 0 1 1 0 1 1]\n",
            "True:[0 1 0 1 1 0 0 1]\n",
            "49 + 40 = 91\n",
            "------------\n",
            "iters:4700\n",
            "Loss:0.8896247808677232\n",
            "Pred:[1 0 0 1 0 1 1 0]\n",
            "True:[1 0 0 1 1 0 0 0]\n",
            "77 + 75 = 150\n",
            "------------\n",
            "iters:4800\n",
            "Loss:0.5567557437117322\n",
            "Pred:[1 0 1 0 0 1 0 0]\n",
            "True:[1 0 0 0 0 1 0 0]\n",
            "84 + 48 = 164\n",
            "------------\n",
            "iters:4900\n",
            "Loss:0.5163699250201363\n",
            "Pred:[1 0 0 1 1 0 0 1]\n",
            "True:[1 1 0 1 1 0 0 1]\n",
            "105 + 112 = 153\n",
            "------------\n",
            "iters:5000\n",
            "Loss:0.21104807409909204\n",
            "Pred:[0 0 1 1 1 0 1 0]\n",
            "True:[0 0 1 1 1 0 1 0]\n",
            "1 + 57 = 58\n",
            "------------\n",
            "iters:5100\n",
            "Loss:0.3506776692611058\n",
            "Pred:[0 0 1 1 0 1 1 0]\n",
            "True:[0 0 1 1 0 1 0 0]\n",
            "51 + 1 = 54\n",
            "------------\n",
            "iters:5200\n",
            "Loss:0.3636488917974048\n",
            "Pred:[0 0 0 0 1 1 1 0]\n",
            "True:[1 0 0 0 1 1 1 0]\n",
            "65 + 77 = 14\n",
            "------------\n",
            "iters:5300\n",
            "Loss:1.3276224469432882\n",
            "Pred:[1 1 1 0 1 1 1 0]\n",
            "True:[1 0 1 1 0 0 0 0]\n",
            "53 + 123 = 238\n",
            "------------\n",
            "iters:5400\n",
            "Loss:0.6054471047241649\n",
            "Pred:[1 1 1 1 1 1 1 0]\n",
            "True:[0 1 1 1 1 1 1 0]\n",
            "108 + 18 = 254\n",
            "------------\n",
            "iters:5500\n",
            "Loss:0.3738320989638053\n",
            "Pred:[1 0 0 1 0 0 1 1]\n",
            "True:[1 1 0 1 0 0 1 1]\n",
            "104 + 107 = 147\n",
            "------------\n",
            "iters:5600\n",
            "Loss:0.7148303837441213\n",
            "Pred:[0 1 1 1 0 0 0 1]\n",
            "True:[0 1 0 0 0 0 0 1]\n",
            "15 + 50 = 113\n",
            "------------\n",
            "iters:5700\n",
            "Loss:0.5985602169703133\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[0 0 1 1 1 1 1 1]\n",
            "24 + 39 = 127\n",
            "------------\n",
            "iters:5800\n",
            "Loss:0.31477435795660347\n",
            "Pred:[1 0 0 0 1 1 0 1]\n",
            "True:[1 0 0 0 1 1 0 1]\n",
            "61 + 80 = 141\n",
            "------------\n",
            "iters:5900\n",
            "Loss:0.1887030167983848\n",
            "Pred:[0 0 1 1 1 0 0 1]\n",
            "True:[0 0 1 1 1 0 0 1]\n",
            "34 + 23 = 57\n",
            "------------\n",
            "iters:6000\n",
            "Loss:0.20712179051058438\n",
            "Pred:[0 1 1 1 0 1 1 0]\n",
            "True:[0 1 1 1 0 1 0 0]\n",
            "81 + 35 = 118\n",
            "------------\n",
            "iters:6100\n",
            "Loss:0.3492907563582796\n",
            "Pred:[1 0 0 0 1 0 0 0]\n",
            "True:[1 1 0 0 1 0 0 0]\n",
            "103 + 97 = 136\n",
            "------------\n",
            "iters:6200\n",
            "Loss:0.4073878816579366\n",
            "Pred:[1 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 0 0 0 0]\n",
            "104 + 88 = 128\n",
            "------------\n",
            "iters:6300\n",
            "Loss:0.2665870932676147\n",
            "Pred:[1 1 0 1 0 0 1 1]\n",
            "True:[1 1 0 1 0 0 1 1]\n",
            "127 + 84 = 211\n",
            "------------\n",
            "iters:6400\n",
            "Loss:0.36082583609403035\n",
            "Pred:[0 1 1 1 1 1 0 0]\n",
            "True:[0 1 1 1 0 0 0 0]\n",
            "102 + 10 = 124\n",
            "------------\n",
            "iters:6500\n",
            "Loss:0.20847535490483707\n",
            "Pred:[0 1 1 1 0 1 1 1]\n",
            "True:[0 1 1 1 0 1 1 1]\n",
            "110 + 9 = 119\n",
            "------------\n",
            "iters:6600\n",
            "Loss:0.11686670282916779\n",
            "Pred:[0 1 0 0 0 1 0 1]\n",
            "True:[0 1 0 0 0 1 0 1]\n",
            "46 + 23 = 69\n",
            "------------\n",
            "iters:6700\n",
            "Loss:0.30128505398328936\n",
            "Pred:[1 0 0 0 0 0 1 0]\n",
            "True:[1 0 0 0 0 0 1 0]\n",
            "5 + 125 = 130\n",
            "------------\n",
            "iters:6800\n",
            "Loss:0.12481464850388974\n",
            "Pred:[1 0 0 0 0 0 0 1]\n",
            "True:[1 0 0 0 0 0 0 1]\n",
            "101 + 28 = 129\n",
            "------------\n",
            "iters:6900\n",
            "Loss:0.047948490434462886\n",
            "Pred:[1 1 0 1 1 1 1 0]\n",
            "True:[1 1 0 1 1 1 1 0]\n",
            "126 + 96 = 222\n",
            "------------\n",
            "iters:7000\n",
            "Loss:0.14087212873120547\n",
            "Pred:[0 1 1 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 0 0 0]\n",
            "51 + 45 = 96\n",
            "------------\n",
            "iters:7100\n",
            "Loss:0.04915646170812766\n",
            "Pred:[0 1 1 1 1 0 1 1]\n",
            "True:[0 1 1 1 1 0 1 1]\n",
            "69 + 54 = 123\n",
            "------------\n",
            "iters:7200\n",
            "Loss:0.04369948861998546\n",
            "Pred:[1 0 1 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 0 0 0]\n",
            "75 + 85 = 160\n",
            "------------\n",
            "iters:7300\n",
            "Loss:0.03371430705519627\n",
            "Pred:[0 1 1 1 0 1 0 0]\n",
            "True:[0 1 1 1 0 1 0 0]\n",
            "47 + 69 = 116\n",
            "------------\n",
            "iters:7400\n",
            "Loss:0.04261005215551429\n",
            "Pred:[1 0 1 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 0 0 0]\n",
            "104 + 56 = 160\n",
            "------------\n",
            "iters:7500\n",
            "Loss:0.00292387681027758\n",
            "Pred:[0 1 0 0 0 1 1 0]\n",
            "True:[0 1 0 0 0 1 1 0]\n",
            "69 + 1 = 70\n",
            "------------\n",
            "iters:7600\n",
            "Loss:0.028971752155368728\n",
            "Pred:[0 1 1 1 0 1 0 1]\n",
            "True:[0 1 1 1 0 1 0 1]\n",
            "22 + 95 = 117\n",
            "------------\n",
            "iters:7700\n",
            "Loss:0.03692502394443852\n",
            "Pred:[1 1 0 0 1 0 0 1]\n",
            "True:[1 1 0 0 1 0 0 1]\n",
            "74 + 127 = 201\n",
            "------------\n",
            "iters:7800\n",
            "Loss:0.05325811610013888\n",
            "Pred:[1 1 0 0 0 0 1 0]\n",
            "True:[1 1 0 0 0 0 1 0]\n",
            "71 + 123 = 194\n",
            "------------\n",
            "iters:7900\n",
            "Loss:0.012901642118344821\n",
            "Pred:[1 0 1 1 0 1 0 1]\n",
            "True:[1 0 1 1 0 1 0 1]\n",
            "66 + 115 = 181\n",
            "------------\n",
            "iters:8000\n",
            "Loss:0.026007335768025868\n",
            "Pred:[0 1 1 0 0 0 1 0]\n",
            "True:[0 1 1 0 0 0 1 0]\n",
            "15 + 83 = 98\n",
            "------------\n",
            "iters:8100\n",
            "Loss:0.019264525300443952\n",
            "Pred:[1 1 0 0 1 0 1 1]\n",
            "True:[1 1 0 0 1 0 1 1]\n",
            "89 + 114 = 203\n",
            "------------\n",
            "iters:8200\n",
            "Loss:0.0181627744049125\n",
            "Pred:[1 0 0 1 1 0 0 0]\n",
            "True:[1 0 0 1 1 0 0 0]\n",
            "37 + 115 = 152\n",
            "------------\n",
            "iters:8300\n",
            "Loss:0.01553585782122778\n",
            "Pred:[1 0 1 0 0 0 0 1]\n",
            "True:[1 0 1 0 0 0 0 1]\n",
            "61 + 100 = 161\n",
            "------------\n",
            "iters:8400\n",
            "Loss:0.014370033494975058\n",
            "Pred:[1 0 1 1 1 1 1 1]\n",
            "True:[1 0 1 1 1 1 1 1]\n",
            "112 + 79 = 191\n",
            "------------\n",
            "iters:8500\n",
            "Loss:0.004054884144004149\n",
            "Pred:[0 1 0 1 0 0 1 0]\n",
            "True:[0 1 0 1 0 0 1 0]\n",
            "29 + 53 = 82\n",
            "------------\n",
            "iters:8600\n",
            "Loss:0.01194167465295323\n",
            "Pred:[1 0 0 0 0 1 0 1]\n",
            "True:[1 0 0 0 0 1 0 1]\n",
            "17 + 116 = 133\n",
            "------------\n",
            "iters:8700\n",
            "Loss:0.013812628886145994\n",
            "Pred:[0 1 0 1 0 1 1 1]\n",
            "True:[0 1 0 1 0 1 1 1]\n",
            "69 + 18 = 87\n",
            "------------\n",
            "iters:8800\n",
            "Loss:0.00918652912730766\n",
            "Pred:[0 1 1 1 1 0 0 1]\n",
            "True:[0 1 1 1 1 0 0 1]\n",
            "92 + 29 = 121\n",
            "------------\n",
            "iters:8900\n",
            "Loss:0.00834766007043492\n",
            "Pred:[0 1 1 0 0 1 0 1]\n",
            "True:[0 1 1 0 0 1 0 1]\n",
            "75 + 26 = 101\n",
            "------------\n",
            "iters:9000\n",
            "Loss:0.007734807482712125\n",
            "Pred:[0 1 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 0 0 0 0]\n",
            "29 + 35 = 64\n",
            "------------\n",
            "iters:9100\n",
            "Loss:0.012014032863612623\n",
            "Pred:[1 1 0 0 0 0 1 1]\n",
            "True:[1 1 0 0 0 0 1 1]\n",
            "125 + 70 = 195\n",
            "------------\n",
            "iters:9200\n",
            "Loss:0.0010127774570830655\n",
            "Pred:[1 1 0 1 0 1 1 0]\n",
            "True:[1 1 0 1 0 1 1 0]\n",
            "101 + 113 = 214\n",
            "------------\n",
            "iters:9300\n",
            "Loss:0.005351147296942161\n",
            "Pred:[1 0 0 1 1 0 0 1]\n",
            "True:[1 0 0 1 1 0 0 1]\n",
            "48 + 105 = 153\n",
            "------------\n",
            "iters:9400\n",
            "Loss:0.001652952448445853\n",
            "Pred:[0 1 1 0 1 1 1 0]\n",
            "True:[0 1 1 0 1 1 1 0]\n",
            "85 + 25 = 110\n",
            "------------\n",
            "iters:9500\n",
            "Loss:0.007803740769805135\n",
            "Pred:[1 0 0 0 1 1 0 1]\n",
            "True:[1 0 0 0 1 1 0 1]\n",
            "123 + 18 = 141\n",
            "------------\n",
            "iters:9600\n",
            "Loss:0.016879984620705443\n",
            "Pred:[1 1 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 0 0 0 0]\n",
            "106 + 86 = 192\n",
            "------------\n",
            "iters:9700\n",
            "Loss:0.007978910028485416\n",
            "Pred:[1 1 0 1 1 0 1 1]\n",
            "True:[1 1 0 1 1 0 1 1]\n",
            "92 + 127 = 219\n",
            "------------\n",
            "iters:9800\n",
            "Loss:0.000904886490733886\n",
            "Pred:[1 1 0 1 0 1 1 0]\n",
            "True:[1 1 0 1 0 1 1 0]\n",
            "113 + 101 = 214\n",
            "------------\n",
            "iters:9900\n",
            "Loss:0.0014827174325850363\n",
            "Pred:[0 0 1 1 0 1 0 0]\n",
            "True:[0 0 1 1 0 1 0 0]\n",
            "33 + 19 = 52\n",
            "------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7zQEPrtP4OP"
      },
      "source": [
        "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "## [try] weight_init_stdやlearning_rate, hidden_layer_sizeを変更してみよう\n",
        "\n",
        "\n",
        "## [try] 重みの初期化方法を変更してみよう\n",
        "Xavier, He\n",
        "\n",
        "## [try] 中間層の活性化関数を変更してみよう\n",
        "ReLU(勾配爆発を確認しよう)<br>\n",
        "tanh(numpyにtanhが用意されている。導関数をd_tanhとして作成しよう)\n",
        "\n",
        "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q66tJ1rJCwL4"
      },
      "source": [
        "## 中間層の活性化関数をReLUに変更する事によって勾配爆発を確認する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5BKw2zumRQv"
      },
      "source": [
        "中間層の活性化関数をReLUに変更すると勾配爆発によって学習ができなくなる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oESp45VmCt7m",
        "outputId": "fdaa4de2-3a6f-4260-c88f-062109f77b58"
      },
      "source": [
        "import numpy as np\n",
        "from common import functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def d_tanh(x):\n",
        "  sinh = (np.exp(x) - np.exp(-x)) / 2\n",
        "  cosh = (np.exp(x) + np.exp(-x)) / 2\n",
        "  return (cosh**2 - sinh**2) / cosh**2\n",
        "\n",
        "# データを用意\n",
        "# 2進数の桁数\n",
        "binary_dim = 8\n",
        "# 最大値 + 1\n",
        "largest_number = pow(2, binary_dim)\n",
        "# largest_numberまで2進数を用意\n",
        "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
        "\n",
        "input_layer_size = 2\n",
        "hidden_layer_size = 16\n",
        "output_layer_size = 1\n",
        "\n",
        "weight_init_std = 1\n",
        "learning_rate = 0.1\n",
        "\n",
        "iters_num = 10000\n",
        "plot_interval = 100\n",
        "\n",
        "# ウェイト初期化 (バイアスは簡単のため省略)\n",
        "W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
        "W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
        "W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n",
        "\n",
        "# Xavier\n",
        "# W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size))\n",
        "# W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "# W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "\n",
        "# He\n",
        "# W_in = np.random.randn(input_layer_size, hidden_layer_size) / np.sqrt(input_layer_size) * np.sqrt(2)\n",
        "# W_out = np.random.randn(hidden_layer_size, output_layer_size) / np.sqrt(hidden_layer_size) * np.sqrt(2)\n",
        "# W = np.random.randn(hidden_layer_size, hidden_layer_size) / np.sqrt(hidden_layer_size) * np.sqrt(2)\n",
        "\n",
        "\n",
        "# 勾配\n",
        "W_in_grad = np.zeros_like(W_in)\n",
        "W_out_grad = np.zeros_like(W_out)\n",
        "W_grad = np.zeros_like(W)\n",
        "\n",
        "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "y = np.zeros((output_layer_size, binary_dim))\n",
        "\n",
        "delta_out = np.zeros((output_layer_size, binary_dim))\n",
        "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "\n",
        "all_losses = []\n",
        "\n",
        "for i in range(iters_num):\n",
        "    \n",
        "    # A, B初期化 (a + b = d)\n",
        "    a_int = np.random.randint(largest_number/2)\n",
        "    a_bin = binary[a_int] # binary encoding\n",
        "    b_int = np.random.randint(largest_number/2)\n",
        "    b_bin = binary[b_int] # binary encoding\n",
        "    \n",
        "    # 正解データ\n",
        "    d_int = a_int + b_int\n",
        "    d_bin = binary[d_int]\n",
        "    \n",
        "    # 出力バイナリ\n",
        "    out_bin = np.zeros_like(d_bin)\n",
        "    \n",
        "    # 時系列全体の誤差\n",
        "    all_loss = 0    \n",
        "    \n",
        "    # 時系列ループ\n",
        "    for t in range(binary_dim):\n",
        "        # 入力値\n",
        "        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n",
        "        # 時刻tにおける正解データ\n",
        "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
        "        \n",
        "        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
        "        # z[:,t+1] = functions.sigmoid(u[:,t+1])\n",
        "        z[:,t+1] = functions.relu(u[:,t+1])\n",
        "\n",
        "        # y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "        y[:,t] = functions.relu(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "\n",
        "        #誤差\n",
        "        loss = functions.mean_squared_error(dd, y[:,t])\n",
        "        \n",
        "        # delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_sigmoid(y[:,t])        \n",
        "        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_relu(y[:,t])    \n",
        "\n",
        "        all_loss += loss\n",
        "\n",
        "        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n",
        "    \n",
        "    \n",
        "    for t in range(binary_dim)[::-1]:\n",
        "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
        "\n",
        "        # delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])\n",
        "        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_relu(u[:,t+1])\n",
        "\n",
        "        # 勾配更新\n",
        "        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n",
        "        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n",
        "        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n",
        "    \n",
        "    # 勾配適用\n",
        "    W_in -= learning_rate * W_in_grad\n",
        "    W_out -= learning_rate * W_out_grad\n",
        "    W -= learning_rate * W_grad\n",
        "    \n",
        "    W_in_grad *= 0\n",
        "    W_out_grad *= 0\n",
        "    W_grad *= 0\n",
        "    \n",
        "\n",
        "    if(i % plot_interval == 0):\n",
        "        all_losses.append(all_loss)        \n",
        "        print(\"iters:\" + str(i))\n",
        "        print(\"Loss:\" + str(all_loss))\n",
        "        print(\"Pred:\" + str(out_bin))\n",
        "        print(\"True:\" + str(d_bin))\n",
        "        out_int = 0\n",
        "        for index,x in enumerate(reversed(out_bin)):\n",
        "            out_int += x * pow(2, index)\n",
        "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n",
        "        print(\"------------\")\n",
        "\n",
        "lists = range(0, iters_num, plot_interval)\n",
        "plt.plot(lists, all_losses, label=\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iters:0\n",
            "Loss:813.5084572821481\n",
            "Pred:[ 0  0  0 26 28 16  0  1]\n",
            "True:[0 0 1 0 1 1 1 1]\n",
            "6 + 41 = 705\n",
            "------------\n",
            "iters:100\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 1 0 1 0]\n",
            "59 + 31 = 0\n",
            "------------\n",
            "iters:200\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 0 1 1]\n",
            "1 + 114 = 0\n",
            "------------\n",
            "iters:300\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 0 1 1 0 1 0]\n",
            "22 + 4 = 0\n",
            "------------\n",
            "iters:400\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 1 0 1 0]\n",
            "74 + 16 = 0\n",
            "------------\n",
            "iters:500\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 1 1 1]\n",
            "28 + 75 = 0\n",
            "------------\n",
            "iters:600\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 1 1 0 0]\n",
            "52 + 24 = 0\n",
            "------------\n",
            "iters:700\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 1 0 1 0 1]\n",
            "22 + 31 = 0\n",
            "------------\n",
            "iters:800\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 1 0 0 0]\n",
            "120 + 48 = 0\n",
            "------------\n",
            "iters:900\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 1 0 0 0]\n",
            "19 + 53 = 0\n",
            "------------\n",
            "iters:1000\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 1 1 0 1 0]\n",
            "127 + 91 = 0\n",
            "------------\n",
            "iters:1100\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 0 1 1 0]\n",
            "101 + 81 = 0\n",
            "------------\n",
            "iters:1200\n",
            "Loss:3.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 1 1 1 1]\n",
            "80 + 95 = 0\n",
            "------------\n",
            "iters:1300\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 0 1 1]\n",
            "110 + 5 = 0\n",
            "------------\n",
            "iters:1400\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 1 0 1 1]\n",
            "19 + 72 = 0\n",
            "------------\n",
            "iters:1500\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 1 0 0 1 0 1]\n",
            "108 + 121 = 0\n",
            "------------\n",
            "iters:1600\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 0 0 1]\n",
            "57 + 24 = 0\n",
            "------------\n",
            "iters:1700\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 0 0 1]\n",
            "6 + 99 = 0\n",
            "------------\n",
            "iters:1800\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 0 0 1 1 1 0]\n",
            "0 + 14 = 0\n",
            "------------\n",
            "iters:1900\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 1 1 1 0]\n",
            "105 + 53 = 0\n",
            "------------\n",
            "iters:2000\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 0 0 0]\n",
            "82 + 78 = 0\n",
            "------------\n",
            "iters:2100\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 1 1 0 0]\n",
            "18 + 58 = 0\n",
            "------------\n",
            "iters:2200\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 0 1 0 0]\n",
            "115 + 65 = 0\n",
            "------------\n",
            "iters:2300\n",
            "Loss:3.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 1 1 1 1 1]\n",
            "52 + 11 = 0\n",
            "------------\n",
            "iters:2400\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 1 1 1 1]\n",
            "106 + 37 = 0\n",
            "------------\n",
            "iters:2500\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 1 0 0 0]\n",
            "108 + 76 = 0\n",
            "------------\n",
            "iters:2600\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 0 0 0 0]\n",
            "61 + 115 = 0\n",
            "------------\n",
            "iters:2700\n",
            "Loss:0.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 0 0]\n",
            "12 + 116 = 0\n",
            "------------\n",
            "iters:2800\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 0 1 0 1]\n",
            "111 + 86 = 0\n",
            "------------\n",
            "iters:2900\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 0 1 0 1]\n",
            "118 + 79 = 0\n",
            "------------\n",
            "iters:3000\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 1 1 0 0]\n",
            "66 + 58 = 0\n",
            "------------\n",
            "iters:3100\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 0 1 0]\n",
            "50 + 32 = 0\n",
            "------------\n",
            "iters:3200\n",
            "Loss:3.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 1 1 1 1]\n",
            "92 + 115 = 0\n",
            "------------\n",
            "iters:3300\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 0 1 0]\n",
            "58 + 40 = 0\n",
            "------------\n",
            "iters:3400\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 1 0 1 0]\n",
            "34 + 104 = 0\n",
            "------------\n",
            "iters:3500\n",
            "Loss:3.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 1 1 1 1 1]\n",
            "112 + 111 = 0\n",
            "------------\n",
            "iters:3600\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 1 0 0 1 1]\n",
            "40 + 11 = 0\n",
            "------------\n",
            "iters:3700\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 0 0 0]\n",
            "72 + 24 = 0\n",
            "------------\n",
            "iters:3800\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 0 0 0 1 1]\n",
            "2 + 33 = 0\n",
            "------------\n",
            "iters:3900\n",
            "Loss:3.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 1 1 0 1]\n",
            "102 + 87 = 0\n",
            "------------\n",
            "iters:4000\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 0 1 0 0]\n",
            "113 + 67 = 0\n",
            "------------\n",
            "iters:4100\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 0 0 0]\n",
            "58 + 38 = 0\n",
            "------------\n",
            "iters:4200\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 0 0 1]\n",
            "0 + 105 = 0\n",
            "------------\n",
            "iters:4300\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 0 1]\n",
            "34 + 95 = 0\n",
            "------------\n",
            "iters:4400\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 1 1 0]\n",
            "11 + 75 = 0\n",
            "------------\n",
            "iters:4500\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 0 1 0 1]\n",
            "119 + 62 = 0\n",
            "------------\n",
            "iters:4600\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 1 0 0 0 0]\n",
            "85 + 123 = 0\n",
            "------------\n",
            "iters:4700\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 0 0 0 1]\n",
            "37 + 28 = 0\n",
            "------------\n",
            "iters:4800\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 1 1 1]\n",
            "76 + 27 = 0\n",
            "------------\n",
            "iters:4900\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 1 0 1 0]\n",
            "48 + 26 = 0\n",
            "------------\n",
            "iters:5000\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 1 1 0]\n",
            "45 + 121 = 0\n",
            "------------\n",
            "iters:5100\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 1 0 0]\n",
            "102 + 30 = 0\n",
            "------------\n",
            "iters:5200\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 1 0 1 1]\n",
            "16 + 59 = 0\n",
            "------------\n",
            "iters:5300\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 1 0 0]\n",
            "37 + 111 = 0\n",
            "------------\n",
            "iters:5400\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 1 0 0 1 0]\n",
            "83 + 127 = 0\n",
            "------------\n",
            "iters:5500\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 0 0 1 1]\n",
            "89 + 90 = 0\n",
            "------------\n",
            "iters:5600\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 0 1 0 1]\n",
            "84 + 113 = 0\n",
            "------------\n",
            "iters:5700\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 1 1 0]\n",
            "13 + 105 = 0\n",
            "------------\n",
            "iters:5800\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 1 1 1 0 0]\n",
            "123 + 97 = 0\n",
            "------------\n",
            "iters:5900\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 1 0 1 0]\n",
            "27 + 47 = 0\n",
            "------------\n",
            "iters:6000\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 1 0 0 0]\n",
            "121 + 47 = 0\n",
            "------------\n",
            "iters:6100\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 1 0 0 1]\n",
            "14 + 107 = 0\n",
            "------------\n",
            "iters:6200\n",
            "Loss:3.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 1 1 1 1]\n",
            "42 + 53 = 0\n",
            "------------\n",
            "iters:6300\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 1 1 1]\n",
            "74 + 13 = 0\n",
            "------------\n",
            "iters:6400\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 1 0 1]\n",
            "118 + 31 = 0\n",
            "------------\n",
            "iters:6500\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 1 1 0]\n",
            "49 + 61 = 0\n",
            "------------\n",
            "iters:6600\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 0 1 1]\n",
            "5 + 110 = 0\n",
            "------------\n",
            "iters:6700\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 1 0 1 0]\n",
            "96 + 42 = 0\n",
            "------------\n",
            "iters:6800\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 1 0 0 0 1 0]\n",
            "109 + 117 = 0\n",
            "------------\n",
            "iters:6900\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 1 1 1]\n",
            "104 + 31 = 0\n",
            "------------\n",
            "iters:7000\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 0 0 0]\n",
            "1 + 95 = 0\n",
            "------------\n",
            "iters:7100\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 0 0 0]\n",
            "123 + 37 = 0\n",
            "------------\n",
            "iters:7200\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 0 1 0]\n",
            "3 + 103 = 0\n",
            "------------\n",
            "iters:7300\n",
            "Loss:3.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 1 1 1 0]\n",
            "78 + 112 = 0\n",
            "------------\n",
            "iters:7400\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 1 0 0]\n",
            "72 + 36 = 0\n",
            "------------\n",
            "iters:7500\n",
            "Loss:3.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 1 1 1 1]\n",
            "56 + 119 = 0\n",
            "------------\n",
            "iters:7600\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 1 1 0]\n",
            "15 + 95 = 0\n",
            "------------\n",
            "iters:7700\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 0 0 0]\n",
            "110 + 2 = 0\n",
            "------------\n",
            "iters:7800\n",
            "Loss:3.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 1 1 1 1]\n",
            "126 + 81 = 0\n",
            "------------\n",
            "iters:7900\n",
            "Loss:3.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 1 1 1 1]\n",
            "109 + 50 = 0\n",
            "------------\n",
            "iters:8000\n",
            "Loss:3.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 1 1 1 1 1]\n",
            "12 + 51 = 0\n",
            "------------\n",
            "iters:8100\n",
            "Loss:3.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 1 1 1 1]\n",
            "55 + 120 = 0\n",
            "------------\n",
            "iters:8200\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 1 0 0 0]\n",
            "93 + 43 = 0\n",
            "------------\n",
            "iters:8300\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 0 1 1]\n",
            "11 + 88 = 0\n",
            "------------\n",
            "iters:8400\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 1 1 0 1]\n",
            "82 + 59 = 0\n",
            "------------\n",
            "iters:8500\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 1 1 1 1 0]\n",
            "28 + 34 = 0\n",
            "------------\n",
            "iters:8600\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 1 1 1 0]\n",
            "40 + 54 = 0\n",
            "------------\n",
            "iters:8700\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 1 1 0 0 1]\n",
            "118 + 99 = 0\n",
            "------------\n",
            "iters:8800\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 1 0 1 1 0]\n",
            "23 + 31 = 0\n",
            "------------\n",
            "iters:8900\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 1 1 1]\n",
            "102 + 1 = 0\n",
            "------------\n",
            "iters:9000\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 1 0 0 1 0]\n",
            "105 + 105 = 0\n",
            "------------\n",
            "iters:9100\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 1 1 1 0]\n",
            "62 + 112 = 0\n",
            "------------\n",
            "iters:9200\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 0 0 1]\n",
            "63 + 82 = 0\n",
            "------------\n",
            "iters:9300\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 1 1 0]\n",
            "94 + 40 = 0\n",
            "------------\n",
            "iters:9400\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 1 0 1 0 0 0]\n",
            "126 + 106 = 0\n",
            "------------\n",
            "iters:9500\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 1 0 1 1]\n",
            "52 + 39 = 0\n",
            "------------\n",
            "iters:9600\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 1 0 0 0 1]\n",
            "116 + 93 = 0\n",
            "------------\n",
            "iters:9700\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 1 1 1 0]\n",
            "102 + 40 = 0\n",
            "------------\n",
            "iters:9800\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 0 1 1]\n",
            "78 + 85 = 0\n",
            "------------\n",
            "iters:9900\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 1 0 1]\n",
            "102 + 47 = 0\n",
            "------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVeItBY_Xkgu"
      },
      "source": [
        "## tanh(numpyにtanhが用意されている。導関数をd_tanhとして作成しよう)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Pkbrz48njAJ"
      },
      "source": [
        "中間層の活性化関数をtanhに変更し（かつHeの初期値を使用すると）かなり学習がスムーズに行われる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rghszVGkPhSL",
        "outputId": "947b25de-3ebb-4729-9e7b-f038b0223c63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "from common import functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def d_tanh(x):\n",
        "  sinh = (np.exp(x) - np.exp(-x)) / 2\n",
        "  cosh = (np.exp(x) + np.exp(-x)) / 2\n",
        "  return (cosh**2 - sinh**2) / cosh**2\n",
        "\n",
        "# データを用意\n",
        "# 2進数の桁数\n",
        "binary_dim = 8\n",
        "# 最大値 + 1\n",
        "largest_number = pow(2, binary_dim)\n",
        "# largest_numberまで2進数を用意\n",
        "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
        "\n",
        "input_layer_size = 2\n",
        "hidden_layer_size = 16\n",
        "output_layer_size = 1\n",
        "\n",
        "weight_init_std = 1\n",
        "learning_rate = 0.1\n",
        "\n",
        "iters_num = 10000\n",
        "plot_interval = 100\n",
        "\n",
        "# ウェイト初期化 (バイアスは簡単のため省略)\n",
        "W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
        "W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
        "W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n",
        "\n",
        "# Xavier\n",
        "\n",
        "\n",
        "# He\n",
        "W_in = np.random.randn(input_layer_size, hidden_layer_size) / np.sqrt(input_layer_size) * np.sqrt(2)\n",
        "W_out = np.random.randn(hidden_layer_size, output_layer_size) / np.sqrt(hidden_layer_size) * np.sqrt(2)\n",
        "W = np.random.randn(hidden_layer_size, hidden_layer_size) / np.sqrt(hidden_layer_size) * np.sqrt(2)\n",
        "\n",
        "\n",
        "# 勾配\n",
        "W_in_grad = np.zeros_like(W_in)\n",
        "W_out_grad = np.zeros_like(W_out)\n",
        "W_grad = np.zeros_like(W)\n",
        "\n",
        "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "y = np.zeros((output_layer_size, binary_dim))\n",
        "\n",
        "delta_out = np.zeros((output_layer_size, binary_dim))\n",
        "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "\n",
        "all_losses = []\n",
        "\n",
        "for i in range(iters_num):\n",
        "    \n",
        "    # A, B初期化 (a + b = d)\n",
        "    a_int = np.random.randint(largest_number/2)\n",
        "    a_bin = binary[a_int] # binary encoding\n",
        "    b_int = np.random.randint(largest_number/2)\n",
        "    b_bin = binary[b_int] # binary encoding\n",
        "    \n",
        "    # 正解データ\n",
        "    d_int = a_int + b_int\n",
        "    d_bin = binary[d_int]\n",
        "    \n",
        "    # 出力バイナリ\n",
        "    out_bin = np.zeros_like(d_bin)\n",
        "    \n",
        "    # 時系列全体の誤差\n",
        "    all_loss = 0    \n",
        "    \n",
        "    # 時系列ループ\n",
        "    for t in range(binary_dim):\n",
        "        # 入力値\n",
        "        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n",
        "        # 時刻tにおける正解データ\n",
        "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
        "        \n",
        "        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
        "        # z[:,t+1] = functions.sigmoid(u[:,t+1])\n",
        "        z[:,t+1] = np.tanh(u[:,t+1])\n",
        "\n",
        "        # y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "        y[:,t] = np.tanh(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "\n",
        "        #誤差\n",
        "        loss = functions.mean_squared_error(dd, y[:,t])\n",
        "        \n",
        "        # delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_sigmoid(y[:,t])        \n",
        "        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * d_tanh(y[:,t])    \n",
        "\n",
        "        all_loss += loss\n",
        "\n",
        "        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n",
        "    \n",
        "    \n",
        "    for t in range(binary_dim)[::-1]:\n",
        "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
        "\n",
        "        # delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])\n",
        "        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * d_tanh(u[:,t+1])\n",
        "\n",
        "        # 勾配更新\n",
        "        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n",
        "        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n",
        "        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n",
        "    \n",
        "    # 勾配適用\n",
        "    W_in -= learning_rate * W_in_grad\n",
        "    W_out -= learning_rate * W_out_grad\n",
        "    W -= learning_rate * W_grad\n",
        "    \n",
        "    W_in_grad *= 0\n",
        "    W_out_grad *= 0\n",
        "    W_grad *= 0\n",
        "    \n",
        "\n",
        "    if(i % plot_interval == 0):\n",
        "        all_losses.append(all_loss)        \n",
        "        print(\"iters:\" + str(i))\n",
        "        print(\"Loss:\" + str(all_loss))\n",
        "        print(\"Pred:\" + str(out_bin))\n",
        "        print(\"True:\" + str(d_bin))\n",
        "        out_int = 0\n",
        "        for index,x in enumerate(reversed(out_bin)):\n",
        "            out_int += x * pow(2, index)\n",
        "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n",
        "        print(\"------------\")\n",
        "\n",
        "lists = range(0, iters_num, plot_interval)\n",
        "plt.plot(lists, all_losses, label=\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iters:0\n",
            "Loss:1.5255512270821048\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[0 0 0 1 1 0 1 0]\n",
            "13 + 13 = 255\n",
            "------------\n",
            "iters:100\n",
            "Loss:0.884806362486125\n",
            "Pred:[1 0 1 1 0 0 1 0]\n",
            "True:[1 0 1 0 1 0 1 0]\n",
            "73 + 97 = 178\n",
            "------------\n",
            "iters:200\n",
            "Loss:1.1546995189762557\n",
            "Pred:[1 1 1 1 1 1 1 0]\n",
            "True:[0 1 0 1 0 1 0 0]\n",
            "18 + 66 = 254\n",
            "------------\n",
            "iters:300\n",
            "Loss:1.2666783628823797\n",
            "Pred:[0 0 1 1 0 1 0 1]\n",
            "True:[0 1 0 1 1 0 1 1]\n",
            "57 + 34 = 53\n",
            "------------\n",
            "iters:400\n",
            "Loss:0.6895750947315344\n",
            "Pred:[255   1   1   1   1   1   1   0]\n",
            "True:[0 1 1 1 1 0 0 0]\n",
            "14 + 106 = 32766\n",
            "------------\n",
            "iters:500\n",
            "Loss:1.0064350476803812\n",
            "Pred:[0 1 1 0 1 1 0 1]\n",
            "True:[0 1 1 1 0 0 0 1]\n",
            "59 + 54 = 109\n",
            "------------\n",
            "iters:600\n",
            "Loss:0.22524072358061936\n",
            "Pred:[1 0 1 0 0 0 0 1]\n",
            "True:[1 0 1 0 0 0 0 1]\n",
            "88 + 73 = 161\n",
            "------------\n",
            "iters:700\n",
            "Loss:0.6085339327448556\n",
            "Pred:[0 1 1 0 0 0 0 0]\n",
            "True:[0 1 0 0 0 0 0 0]\n",
            "27 + 37 = 96\n",
            "------------\n",
            "iters:800\n",
            "Loss:0.049007056243507374\n",
            "Pred:[0 1 1 0 1 1 1 0]\n",
            "True:[0 1 1 0 1 1 1 0]\n",
            "41 + 69 = 110\n",
            "------------\n",
            "iters:900\n",
            "Loss:0.43123129023381\n",
            "Pred:[1 1 1 1 1 0 1 1]\n",
            "True:[0 1 1 1 1 0 1 1]\n",
            "27 + 96 = 251\n",
            "------------\n",
            "iters:1000\n",
            "Loss:0.08229401411256076\n",
            "Pred:[1 0 0 1 1 0 1 0]\n",
            "True:[1 0 0 1 1 0 1 0]\n",
            "111 + 43 = 154\n",
            "------------\n",
            "iters:1100\n",
            "Loss:0.23742876587317246\n",
            "Pred:[0 1 0 0 0 0 0 1]\n",
            "True:[0 1 0 0 0 0 0 1]\n",
            "37 + 28 = 65\n",
            "------------\n",
            "iters:1200\n",
            "Loss:0.02017038930955504\n",
            "Pred:[1 1 0 0 0 1 1 0]\n",
            "True:[1 1 0 0 0 1 1 0]\n",
            "112 + 86 = 198\n",
            "------------\n",
            "iters:1300\n",
            "Loss:0.12554214297147587\n",
            "Pred:[1 0 0 0 1 0 1 1]\n",
            "True:[1 0 0 0 1 0 1 1]\n",
            "14 + 125 = 139\n",
            "------------\n",
            "iters:1400\n",
            "Loss:0.12928654009173898\n",
            "Pred:[0 0 1 0 0 0 0 1]\n",
            "True:[0 0 1 0 0 0 0 1]\n",
            "21 + 12 = 33\n",
            "------------\n",
            "iters:1500\n",
            "Loss:0.02735390234568818\n",
            "Pred:[0 1 0 1 1 0 1 0]\n",
            "True:[0 1 0 1 1 0 1 0]\n",
            "60 + 30 = 90\n",
            "------------\n",
            "iters:1600\n",
            "Loss:0.05042662741227129\n",
            "Pred:[0 0 0 1 1 1 0 1]\n",
            "True:[0 0 0 1 1 1 0 1]\n",
            "9 + 20 = 29\n",
            "------------\n",
            "iters:1700\n",
            "Loss:0.008195465826201152\n",
            "Pred:[1 0 0 0 1 1 1 1]\n",
            "True:[1 0 0 0 1 1 1 1]\n",
            "45 + 98 = 143\n",
            "------------\n",
            "iters:1800\n",
            "Loss:0.15502450073298935\n",
            "Pred:[1 0 0 1 1 0 0 0]\n",
            "True:[1 0 0 0 1 0 0 0]\n",
            "64 + 72 = 152\n",
            "------------\n",
            "iters:1900\n",
            "Loss:0.018682372104026568\n",
            "Pred:[1 0 0 0 0 0 1 0]\n",
            "True:[1 0 0 0 0 0 1 0]\n",
            "64 + 66 = 130\n",
            "------------\n",
            "iters:2000\n",
            "Loss:0.11612163502486952\n",
            "Pred:[0 1 0 1 0 0 0 1]\n",
            "True:[0 1 0 1 0 0 0 1]\n",
            "3 + 78 = 81\n",
            "------------\n",
            "iters:2100\n",
            "Loss:0.025430148345699195\n",
            "Pred:[0 1 0 1 1 0 0 1]\n",
            "True:[0 1 0 1 1 0 0 1]\n",
            "54 + 35 = 89\n",
            "------------\n",
            "iters:2200\n",
            "Loss:0.0476206623030497\n",
            "Pred:[0 1 1 1 0 1 1 0]\n",
            "True:[0 1 1 1 0 1 1 0]\n",
            "22 + 96 = 118\n",
            "------------\n",
            "iters:2300\n",
            "Loss:0.011360047659899241\n",
            "Pred:[0 1 0 1 1 0 1 1]\n",
            "True:[0 1 0 1 1 0 1 1]\n",
            "24 + 67 = 91\n",
            "------------\n",
            "iters:2400\n",
            "Loss:0.0024177123298596897\n",
            "Pred:[1 1 1 1 1 0 1 1]\n",
            "True:[1 1 1 1 1 0 1 1]\n",
            "125 + 126 = 251\n",
            "------------\n",
            "iters:2500\n",
            "Loss:0.028130881196041582\n",
            "Pred:[0 1 1 1 1 0 1 1]\n",
            "True:[0 1 1 1 1 0 1 1]\n",
            "67 + 56 = 123\n",
            "------------\n",
            "iters:2600\n",
            "Loss:0.016756208560910094\n",
            "Pred:[0 1 0 0 0 0 1 1]\n",
            "True:[0 1 0 0 0 0 1 1]\n",
            "33 + 34 = 67\n",
            "------------\n",
            "iters:2700\n",
            "Loss:0.001396294237497883\n",
            "Pred:[1 0 0 1 0 0 1 0]\n",
            "True:[1 0 0 1 0 0 1 0]\n",
            "73 + 73 = 146\n",
            "------------\n",
            "iters:2800\n",
            "Loss:0.08336242152578989\n",
            "Pred:[0 0 1 1 1 0 0 1]\n",
            "True:[0 0 1 1 1 0 0 1]\n",
            "35 + 22 = 57\n",
            "------------\n",
            "iters:2900\n",
            "Loss:0.010854502315129636\n",
            "Pred:[1 0 0 1 0 0 0 0]\n",
            "True:[1 0 0 1 0 0 0 0]\n",
            "102 + 42 = 144\n",
            "------------\n",
            "iters:3000\n",
            "Loss:0.0022001636935191755\n",
            "Pred:[0 1 1 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 0 0 0]\n",
            "50 + 46 = 96\n",
            "------------\n",
            "iters:3100\n",
            "Loss:0.09270064812224606\n",
            "Pred:[0 1 0 1 1 0 1 1]\n",
            "True:[0 1 0 1 1 0 1 1]\n",
            "16 + 75 = 91\n",
            "------------\n",
            "iters:3200\n",
            "Loss:0.0003973095048210275\n",
            "Pred:[1 1 0 1 1 0 0 0]\n",
            "True:[1 1 0 1 1 0 0 0]\n",
            "119 + 97 = 216\n",
            "------------\n",
            "iters:3300\n",
            "Loss:0.008609428475109404\n",
            "Pred:[1 0 1 0 0 1 0 1]\n",
            "True:[1 0 1 0 0 1 0 1]\n",
            "43 + 122 = 165\n",
            "------------\n",
            "iters:3400\n",
            "Loss:0.017617502121429755\n",
            "Pred:[1 0 0 1 0 1 0 1]\n",
            "True:[1 0 0 1 0 1 0 1]\n",
            "110 + 39 = 149\n",
            "------------\n",
            "iters:3500\n",
            "Loss:0.0016328423822054622\n",
            "Pred:[1 0 1 1 1 0 0 1]\n",
            "True:[1 0 1 1 1 0 0 1]\n",
            "75 + 110 = 185\n",
            "------------\n",
            "iters:3600\n",
            "Loss:0.019196871978064144\n",
            "Pred:[0 1 0 1 1 0 1 1]\n",
            "True:[0 1 0 1 1 0 1 1]\n",
            "24 + 67 = 91\n",
            "------------\n",
            "iters:3700\n",
            "Loss:0.0004751970851669459\n",
            "Pred:[1 1 0 1 1 1 1 0]\n",
            "True:[1 1 0 1 1 1 1 0]\n",
            "107 + 115 = 222\n",
            "------------\n",
            "iters:3800\n",
            "Loss:0.0029891654372491486\n",
            "Pred:[0 1 1 0 1 0 0 1]\n",
            "True:[0 1 1 0 1 0 0 1]\n",
            "14 + 91 = 105\n",
            "------------\n",
            "iters:3900\n",
            "Loss:0.0010569348590018092\n",
            "Pred:[0 0 1 1 0 1 1 1]\n",
            "True:[0 0 1 1 0 1 1 1]\n",
            "2 + 53 = 55\n",
            "------------\n",
            "iters:4000\n",
            "Loss:0.0010714804614564684\n",
            "Pred:[1 0 1 1 0 1 1 0]\n",
            "True:[1 0 1 1 0 1 1 0]\n",
            "60 + 122 = 182\n",
            "------------\n",
            "iters:4100\n",
            "Loss:0.022394828967763264\n",
            "Pred:[0 0 0 1 1 1 0 0]\n",
            "True:[0 0 0 1 1 1 0 0]\n",
            "18 + 10 = 28\n",
            "------------\n",
            "iters:4200\n",
            "Loss:0.005086492041090866\n",
            "Pred:[0 1 1 1 0 1 1 1]\n",
            "True:[0 1 1 1 0 1 1 1]\n",
            "91 + 28 = 119\n",
            "------------\n",
            "iters:4300\n",
            "Loss:0.005883533927439006\n",
            "Pred:[1 0 0 1 1 1 1 1]\n",
            "True:[1 0 0 1 1 1 1 1]\n",
            "105 + 54 = 159\n",
            "------------\n",
            "iters:4400\n",
            "Loss:0.019348307522213268\n",
            "Pred:[0 1 1 1 0 0 0 1]\n",
            "True:[0 1 1 1 0 0 0 1]\n",
            "36 + 77 = 113\n",
            "------------\n",
            "iters:4500\n",
            "Loss:0.00928546192969752\n",
            "Pred:[1 0 1 1 0 0 1 1]\n",
            "True:[1 0 1 1 0 0 1 1]\n",
            "117 + 62 = 179\n",
            "------------\n",
            "iters:4600\n",
            "Loss:0.034522841046155196\n",
            "Pred:[0 1 0 0 1 1 0 1]\n",
            "True:[0 1 0 0 1 1 0 1]\n",
            "60 + 17 = 77\n",
            "------------\n",
            "iters:4700\n",
            "Loss:0.003733520337736738\n",
            "Pred:[1 0 1 1 0 1 0 1]\n",
            "True:[1 0 1 1 0 1 0 1]\n",
            "88 + 93 = 181\n",
            "------------\n",
            "iters:4800\n",
            "Loss:0.006622112978915385\n",
            "Pred:[0 1 1 0 0 1 0 1]\n",
            "True:[0 1 1 0 0 1 0 1]\n",
            "70 + 31 = 101\n",
            "------------\n",
            "iters:4900\n",
            "Loss:0.0056655227350998405\n",
            "Pred:[0 0 1 1 1 0 0 0]\n",
            "True:[0 0 1 1 1 0 0 0]\n",
            "14 + 42 = 56\n",
            "------------\n",
            "iters:5000\n",
            "Loss:0.00046772274920129676\n",
            "Pred:[1 1 1 0 0 0 1 1]\n",
            "True:[1 1 1 0 0 0 1 1]\n",
            "101 + 126 = 227\n",
            "------------\n",
            "iters:5100\n",
            "Loss:0.011419598652544063\n",
            "Pred:[1 0 1 1 1 1 1 0]\n",
            "True:[1 0 1 1 1 1 1 0]\n",
            "123 + 67 = 190\n",
            "------------\n",
            "iters:5200\n",
            "Loss:0.03369768669981385\n",
            "Pred:[0 1 1 1 1 0 0 0]\n",
            "True:[0 1 1 1 1 0 0 0]\n",
            "49 + 71 = 120\n",
            "------------\n",
            "iters:5300\n",
            "Loss:0.004124423418771204\n",
            "Pred:[1 0 0 1 1 1 0 1]\n",
            "True:[1 0 0 1 1 1 0 1]\n",
            "125 + 32 = 157\n",
            "------------\n",
            "iters:5400\n",
            "Loss:0.01675661156294502\n",
            "Pred:[1 0 1 0 1 1 1 0]\n",
            "True:[1 0 1 0 1 1 1 0]\n",
            "107 + 67 = 174\n",
            "------------\n",
            "iters:5500\n",
            "Loss:0.0030972503542219835\n",
            "Pred:[1 0 1 0 1 0 1 0]\n",
            "True:[1 0 1 0 1 0 1 0]\n",
            "70 + 100 = 170\n",
            "------------\n",
            "iters:5600\n",
            "Loss:0.015398704974769428\n",
            "Pred:[0 1 1 1 1 1 0 1]\n",
            "True:[0 1 1 1 1 1 0 1]\n",
            "6 + 119 = 125\n",
            "------------\n",
            "iters:5700\n",
            "Loss:0.00280172996932232\n",
            "Pred:[1 0 1 0 0 0 0 1]\n",
            "True:[1 0 1 0 0 0 0 1]\n",
            "102 + 59 = 161\n",
            "------------\n",
            "iters:5800\n",
            "Loss:0.01351191489378274\n",
            "Pred:[0 0 1 0 1 0 0 0]\n",
            "True:[0 0 1 0 1 0 0 0]\n",
            "5 + 35 = 40\n",
            "------------\n",
            "iters:5900\n",
            "Loss:0.02150849450672166\n",
            "Pred:[1 0 0 1 0 0 0 1]\n",
            "True:[1 0 0 1 0 0 0 1]\n",
            "47 + 98 = 145\n",
            "------------\n",
            "iters:6000\n",
            "Loss:0.007014883762280596\n",
            "Pred:[1 0 0 1 1 0 1 1]\n",
            "True:[1 0 0 1 1 0 1 1]\n",
            "33 + 122 = 155\n",
            "------------\n",
            "iters:6100\n",
            "Loss:0.0014636533573781183\n",
            "Pred:[0 1 1 1 1 0 0 0]\n",
            "True:[0 1 1 1 1 0 0 0]\n",
            "71 + 49 = 120\n",
            "------------\n",
            "iters:6200\n",
            "Loss:0.0006388544746586328\n",
            "Pred:[0 1 1 0 0 1 0 1]\n",
            "True:[0 1 1 0 0 1 0 1]\n",
            "39 + 62 = 101\n",
            "------------\n",
            "iters:6300\n",
            "Loss:0.06122943358400109\n",
            "Pred:[1 0 0 0 0 1 0 1]\n",
            "True:[1 0 0 0 0 1 0 1]\n",
            "7 + 126 = 133\n",
            "------------\n",
            "iters:6400\n",
            "Loss:0.01764270120514346\n",
            "Pred:[0 1 1 0 1 0 1 0]\n",
            "True:[0 1 1 0 1 0 1 0]\n",
            "73 + 33 = 106\n",
            "------------\n",
            "iters:6500\n",
            "Loss:0.002031762365080673\n",
            "Pred:[0 1 0 1 0 1 1 0]\n",
            "True:[0 1 0 1 0 1 1 0]\n",
            "46 + 40 = 86\n",
            "------------\n",
            "iters:6600\n",
            "Loss:0.0007361782051322122\n",
            "Pred:[0 0 0 1 1 1 1 1]\n",
            "True:[0 0 0 1 1 1 1 1]\n",
            "3 + 28 = 31\n",
            "------------\n",
            "iters:6700\n",
            "Loss:0.002223597062147829\n",
            "Pred:[1 1 0 1 1 0 1 1]\n",
            "True:[1 1 0 1 1 0 1 1]\n",
            "120 + 99 = 219\n",
            "------------\n",
            "iters:6800\n",
            "Loss:0.00909872806350207\n",
            "Pred:[1 0 1 1 1 0 1 0]\n",
            "True:[1 0 1 1 1 0 1 0]\n",
            "127 + 59 = 186\n",
            "------------\n",
            "iters:6900\n",
            "Loss:0.00011551084061274228\n",
            "Pred:[0 1 1 1 0 1 0 0]\n",
            "True:[0 1 1 1 0 1 0 0]\n",
            "104 + 12 = 116\n",
            "------------\n",
            "iters:7000\n",
            "Loss:0.0014865517650666713\n",
            "Pred:[1 0 0 0 0 1 1 1]\n",
            "True:[1 0 0 0 0 1 1 1]\n",
            "53 + 82 = 135\n",
            "------------\n",
            "iters:7100\n",
            "Loss:0.0014635532628513696\n",
            "Pred:[1 0 0 0 1 0 1 1]\n",
            "True:[1 0 0 0 1 0 1 1]\n",
            "14 + 125 = 139\n",
            "------------\n",
            "iters:7200\n",
            "Loss:0.0006981597455171513\n",
            "Pred:[1 1 0 0 0 1 1 1]\n",
            "True:[1 1 0 0 0 1 1 1]\n",
            "73 + 126 = 199\n",
            "------------\n",
            "iters:7300\n",
            "Loss:0.001238648528680382\n",
            "Pred:[0 0 1 1 0 1 1 1]\n",
            "True:[0 0 1 1 0 1 1 1]\n",
            "33 + 22 = 55\n",
            "------------\n",
            "iters:7400\n",
            "Loss:0.0017991507828728384\n",
            "Pred:[1 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 0 0]\n",
            "45 + 83 = 128\n",
            "------------\n",
            "iters:7500\n",
            "Loss:0.013665814934168666\n",
            "Pred:[0 0 1 0 0 1 1 1]\n",
            "True:[0 0 1 0 0 1 1 1]\n",
            "32 + 7 = 39\n",
            "------------\n",
            "iters:7600\n",
            "Loss:0.00015185038212686786\n",
            "Pred:[0 1 1 1 1 1 0 1]\n",
            "True:[0 1 1 1 1 1 0 1]\n",
            "87 + 38 = 125\n",
            "------------\n",
            "iters:7700\n",
            "Loss:0.005332126300820275\n",
            "Pred:[0 0 1 1 0 1 1 1]\n",
            "True:[0 0 1 1 0 1 1 1]\n",
            "3 + 52 = 55\n",
            "------------\n",
            "iters:7800\n",
            "Loss:0.0015030836879735964\n",
            "Pred:[0 0 1 1 1 1 1 0]\n",
            "True:[0 0 1 1 1 1 1 0]\n",
            "34 + 28 = 62\n",
            "------------\n",
            "iters:7900\n",
            "Loss:0.008305034017874635\n",
            "Pred:[0 1 0 0 1 0 0 1]\n",
            "True:[0 1 0 0 1 0 0 1]\n",
            "58 + 15 = 73\n",
            "------------\n",
            "iters:8000\n",
            "Loss:0.0005599313264485076\n",
            "Pred:[1 0 1 0 1 1 1 1]\n",
            "True:[1 0 1 0 1 1 1 1]\n",
            "119 + 56 = 175\n",
            "------------\n",
            "iters:8100\n",
            "Loss:0.0008991808567141565\n",
            "Pred:[0 1 0 0 1 1 1 1]\n",
            "True:[0 1 0 0 1 1 1 1]\n",
            "70 + 9 = 79\n",
            "------------\n",
            "iters:8200\n",
            "Loss:0.0005648611914188619\n",
            "Pred:[1 1 0 0 1 1 1 1]\n",
            "True:[1 1 0 0 1 1 1 1]\n",
            "103 + 104 = 207\n",
            "------------\n",
            "iters:8300\n",
            "Loss:0.0008341849566664107\n",
            "Pred:[1 1 0 0 0 1 1 0]\n",
            "True:[1 1 0 0 0 1 1 0]\n",
            "74 + 124 = 198\n",
            "------------\n",
            "iters:8400\n",
            "Loss:0.011764555438264799\n",
            "Pred:[0 0 0 1 0 1 0 0]\n",
            "True:[0 0 0 1 0 1 0 0]\n",
            "10 + 10 = 20\n",
            "------------\n",
            "iters:8500\n",
            "Loss:0.00040420060995762153\n",
            "Pred:[1 1 0 1 1 0 1 1]\n",
            "True:[1 1 0 1 1 0 1 1]\n",
            "103 + 116 = 219\n",
            "------------\n",
            "iters:8600\n",
            "Loss:0.005722018310788496\n",
            "Pred:[0 0 1 0 0 1 0 1]\n",
            "True:[0 0 1 0 0 1 0 1]\n",
            "27 + 10 = 37\n",
            "------------\n",
            "iters:8700\n",
            "Loss:0.0008622486612335253\n",
            "Pred:[1 1 1 0 1 0 0 0]\n",
            "True:[1 1 1 0 1 0 0 0]\n",
            "108 + 124 = 232\n",
            "------------\n",
            "iters:8800\n",
            "Loss:0.0013775457169505404\n",
            "Pred:[0 0 1 0 1 1 0 0]\n",
            "True:[0 0 1 0 1 1 0 0]\n",
            "1 + 43 = 44\n",
            "------------\n",
            "iters:8900\n",
            "Loss:0.0022045618775019334\n",
            "Pred:[0 1 0 0 0 1 1 0]\n",
            "True:[0 1 0 0 0 1 1 0]\n",
            "22 + 48 = 70\n",
            "------------\n",
            "iters:9000\n",
            "Loss:0.0004386794544282041\n",
            "Pred:[1 1 0 0 1 1 1 1]\n",
            "True:[1 1 0 0 1 1 1 1]\n",
            "96 + 111 = 207\n",
            "------------\n",
            "iters:9100\n",
            "Loss:0.0083403167468853\n",
            "Pred:[1 0 0 0 1 1 0 0]\n",
            "True:[1 0 0 0 1 1 0 0]\n",
            "41 + 99 = 140\n",
            "------------\n",
            "iters:9200\n",
            "Loss:0.000554967219800777\n",
            "Pred:[1 1 1 1 0 0 0 0]\n",
            "True:[1 1 1 1 0 0 0 0]\n",
            "117 + 123 = 240\n",
            "------------\n",
            "iters:9300\n",
            "Loss:0.004056059317447688\n",
            "Pred:[1 1 0 0 0 1 1 0]\n",
            "True:[1 1 0 0 0 1 1 0]\n",
            "108 + 90 = 198\n",
            "------------\n",
            "iters:9400\n",
            "Loss:0.001930804806757685\n",
            "Pred:[1 0 0 0 1 0 1 0]\n",
            "True:[1 0 0 0 1 0 1 0]\n",
            "43 + 95 = 138\n",
            "------------\n",
            "iters:9500\n",
            "Loss:0.0005608175807553912\n",
            "Pred:[0 1 0 0 1 1 0 1]\n",
            "True:[0 1 0 0 1 1 0 1]\n",
            "18 + 59 = 77\n",
            "------------\n",
            "iters:9600\n",
            "Loss:0.0027311522819134402\n",
            "Pred:[0 0 1 0 0 0 0 1]\n",
            "True:[0 0 1 0 0 0 0 1]\n",
            "6 + 27 = 33\n",
            "------------\n",
            "iters:9700\n",
            "Loss:0.0012841497576596137\n",
            "Pred:[0 1 1 0 0 1 1 0]\n",
            "True:[0 1 1 0 0 1 1 0]\n",
            "66 + 36 = 102\n",
            "------------\n",
            "iters:9800\n",
            "Loss:0.0016478166694999408\n",
            "Pred:[1 0 1 1 0 1 0 0]\n",
            "True:[1 0 1 1 0 1 0 0]\n",
            "60 + 120 = 180\n",
            "------------\n",
            "iters:9900\n",
            "Loss:5.452025543847022e-05\n",
            "Pred:[1 0 1 1 1 1 0 1]\n",
            "True:[1 0 1 1 1 1 0 1]\n",
            "116 + 73 = 189\n",
            "------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0n0DDV_kYhRs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}